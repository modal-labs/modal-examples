import modal
import os
import re
from typing import List, Optional, Tuple, Dict

# --- é…ç½® ---
MODEL_REPO_ID = "vibevoice/VibeVoice-1.5B"
MODEL_CACHE_DIR = "/cache/models"
GPU_CONFIG = "A100-40GB"

# --- Modal App å®šä¹‰ ---
app = modal.App("vibevoice-tts-final-fixed")
model_cache = modal.Volume.from_name("vibevoice-cache", create_if_missing=True)

# --- ç¯å¢ƒé•œåƒå®šä¹‰ ---
image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install("git", "ffmpeg", "libsndfile1-dev")
    .pip_install(
        "torch",
        "torchaudio",
        "transformers",
        "gradio",
        "soundfile",
        "accelerate",
        "huggingface_hub",
        "packaging",
        "uv",
    )
    .run_commands(
        "git clone https://github.com/vibevoice-community/VibeVoice.git /app || true",
        "cd /app && uv pip install -e . --system",
    )
    .workdir("/app")
)


@app.cls(
    image=image,
    gpu=GPU_CONFIG,
    scaledown_window=300,
    timeout=3600,
    volumes={MODEL_CACHE_DIR: model_cache}
)
class VibeVoiceModel:
    """
    ä¸€ä¸ªå°è£…äº† VibeVoice æ¨¡å‹å’Œå¤„ç†å™¨çš„ç±»ï¼Œç”¨äºç”Ÿæˆè¯­éŸ³ã€‚
    """
    model: object = None
    processor: object = None
    model_path: str = None

    @modal.enter()
    def load_model_and_processor(self):
        """
        åœ¨å®¹å™¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ï¼Œå¹¶å°†å®ƒä»¬ç¼“å­˜åˆ°å®ä¾‹å˜é‡ä¸­ã€‚
        """
        import torch
        import sys
        import os

        try:
            from vibevoice.modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference as VVModel
            from vibevoice.processor.vibevoice_processor import VibeVoiceProcessor
        except ImportError as e:
            print(f"!!! å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
            print(f"    å½“å‰çš„ Python è·¯å¾„ (sys.path): {sys.path}")
            print(f"    /app ç›®å½•ä¸‹çš„å†…å®¹: {os.listdir('/app') if os.path.exists('/app') else 'ä¸å­˜åœ¨'}")
            if os.path.exists('/app/vibevoice'):
                print(f"    /app/vibevoice ç›®å½•ä¸‹çš„å†…å®¹: {os.listdir('/app/vibevoice')}")
            raise

        model_dir = os.path.join(MODEL_CACHE_DIR, MODEL_REPO_ID.split("/")[-1])
        if not os.path.exists(model_dir):
            print(f"æœ¬åœ°ç¼“å­˜ {model_dir} ä¸å­˜åœ¨ï¼Œå¼€å§‹ä» Hugging Face ä¸‹è½½...")
            from huggingface_hub import snapshot_download
            snapshot_download(
                repo_id=MODEL_REPO_ID,
                local_dir=model_dir,
                local_dir_use_symlinks=False,
            )
            model_cache.commit()
            print("æ¨¡å‹ä¸‹è½½å¹¶ç¼“å­˜æˆåŠŸã€‚")
        
        self.model_path = model_dir
        print(f"ä»è·¯å¾„ {self.model_path} åŠ è½½ VibeVoice æ¨¡å‹å’Œå¤„ç†å™¨...")

        self.processor = VibeVoiceProcessor.from_pretrained(self.model_path)
        print("å¤„ç†å™¨åŠ è½½æˆåŠŸã€‚")

        self.model = VVModel.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True 
        )
        self.model.eval()
        print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")

    @modal.method()
    def generate_speech(
        self,
        text: str,
        temperature: float = 0.8,
        top_p: float = 0.9,
    ) -> bytes:
        """
        æ ¹æ®è¾“å…¥æ–‡æœ¬ç”Ÿæˆè¯­éŸ³çš„æ ¸å¿ƒå‡½æ•° (æœ€ç»ˆä¿®å¤ç‰ˆ)ã€‚
        æ­¤ç‰ˆæœ¬ä¸¥æ ¼éµå¾ªç¤¾åŒºä»“åº“ Gradio ç¤ºä¾‹çš„é€»è¾‘ï¼Œä»¥ç¡®ä¿ç¨³å®šæ€§ã€‚
        """
        import torch
        import torchaudio
        import tempfile
        import soundfile as sf
        import os

        if not self.model or not self.processor:
            raise RuntimeError("æ¨¡å‹æˆ–å¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ load_model_and_processor æ–¹æ³•ã€‚")

        try:
            # ========== æ ¸å¿ƒä¿®æ­£ï¼šä¸¥æ ¼éµå¾ªå®˜æ–¹ Gradio ç¤ºä¾‹çš„è°ƒç”¨æ–¹å¼ ==========
            
            lines = text.strip().split('\n')
            if not any(':' in line for line in lines):
                 raise ValueError("è¾“å…¥æ–‡æœ¬æ ¼å¼ä¸æ­£ç¡®ã€‚è¯·ä½¿ç”¨ 'è¯´è¯äºº: å†…å®¹' çš„æ ¼å¼ã€‚")

            # Step 1: ä¸ºæ‰€æœ‰å”¯ä¸€è¯´è¯äººåŠ è½½ä¸€æ¬¡éŸ³é¢‘æ ·æœ¬ï¼Œå­˜å…¥å­—å…¸æ–¹ä¾¿æŸ¥æ‰¾
            speaker_waveforms = {}
            sample_rate = 24000
            
            # å…ˆéå†ä¸€æ¬¡ï¼Œæ‰¾å‡ºæ‰€æœ‰å”¯ä¸€çš„è¯´è¯äºº
            unique_speakers = sorted(list(set(line.split(':', 1)[0].strip() for line in lines if ':' in line)))
            
            for speaker_name in unique_speakers:
                audio_path = f"demo/voices/{speaker_name.lower()}.wav"
                if os.path.exists(audio_path):
                    waveform, sr = torchaudio.load(audio_path)
                    if sr != sample_rate:
                         from torchaudio.transforms import Resample
                         resampler = Resample(sr, sample_rate)
                         waveform = resampler(waveform)
                    speaker_waveforms[speaker_name] = waveform
                else:
                    print(f"Warning: æœªæ‰¾åˆ° {speaker_name} çš„éŸ³é¢‘æ–‡ä»¶ã€‚æ­£åœ¨å†…å­˜ä¸­åˆ›å»ºé»˜è®¤é™éŸ³å¼ é‡ã€‚")
                    speaker_waveforms[speaker_name] = torch.zeros((1, sample_rate), dtype=torch.float32)

            # Step 2: æ„å»ºä¸¤ä¸ªé•¿åº¦å®Œå…¨ç›¸åŒçš„åˆ—è¡¨: texts å’Œ voice_samples_list
            texts_for_processor = []
            voice_samples_for_processor = []

            for line in lines:
                if ':' in line:
                    try:
                        speaker_name, content = line.split(':', 1)
                        speaker_name = speaker_name.strip()
                        content = content.strip()
                        
                        if content: # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
                            texts_for_processor.append(content)
                            voice_samples_for_processor.append(speaker_waveforms[speaker_name])
                    except (ValueError, KeyError) as e:
                        print(f"è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„è¡Œ: '{line}' - é”™è¯¯: {e}")
                        continue
            
            if not texts_for_processor:
                raise ValueError("æ— æ³•ä»è¾“å…¥ä¸­è§£æå‡ºä»»ä½•æœ‰æ•ˆçš„å¯¹è¯è¡Œã€‚")

            print(f"å‡†å¤‡å¥½çš„æ–‡æœ¬è¡Œæ•°: {len(texts_for_processor)}")
            print(f"å‡†å¤‡å¥½çš„éŸ³é¢‘ç‰‡æ®µæ•°: {len(voice_samples_for_processor)}")

            # Step 3: ä½¿ç”¨ "texts" å’Œ "voice_samples_list" å‚æ•°è°ƒç”¨ processor
            inputs = self.processor(
                texts=texts_for_processor, 
                voice_samples_list=voice_samples_for_processor,
                return_tensors="pt"
            )
            # ======================= ä¿®æ­£ç»“æŸ =======================
            
            inputs = {key: val.to(self.model.device) for key, val in inputs.items()}
            print("æ–‡æœ¬å·²æˆåŠŸå¤„ç†ä¸ºè¾“å…¥å¼ é‡ã€‚")

            with torch.no_grad():
                print("è°ƒç”¨ model.generate å¼€å§‹ç”ŸæˆéŸ³é¢‘...")
                audio_output = self.model.generate(**inputs, max_new_tokens=4096, do_sample=True, temperature=temperature, top_p=top_p)
            
            print("éŸ³é¢‘ç”Ÿæˆå®Œæ¯•ã€‚")
            
            waveform_np = audio_output[0].cpu().to(torch.float32).numpy()

            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                sf.write(tmp_file.name, waveform_np, samplerate=sample_rate)
                tmp_file.seek(0)
                audio_bytes = tmp_file.read()
            
            os.unlink(tmp_file.name)
            return audio_bytes

        except Exception as e:
            print(f"ç”Ÿæˆè¯­éŸ³æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            raise

# --- API å’Œ Gradio çš„ä»£ç ä¿æŒä¸å˜ ---

@app.function(image=image, min_containers=1)
@modal.fastapi_endpoint(method="POST")
def api(text_data: dict):
    """ä¸€ä¸ªç®€å•çš„ POST API å…¥å£ç‚¹ï¼Œç”¨äºç¨‹åºåŒ–è°ƒç”¨ã€‚"""
    from fastapi import status
    from fastapi.responses import JSONResponse, Response
    
    text = text_data.get("text")
    if not text:
        return JSONResponse(
            content={"error": "ç¼ºå°‘ 'text' å­—æ®µ"},
            status_code=status.HTTP_400_BAD_REQUEST,
        )
    
    model = VibeVoiceModel()
    try:
        audio_bytes = model.generate_speech.remote(text)
        return Response(content=audio_bytes, media_type="audio/wav")
    except Exception as e:
        # --- æ ¸å¿ƒä¿®å¤ ---
        # 1. å°†å¤æ‚çš„å¼‚å¸¸å¯¹è±¡ e è½¬æ¢ä¸ºä¸€ä¸ªç®€å•çš„å­—ç¬¦ä¸²ã€‚
        error_message = f"åœ¨ç”Ÿæˆè¯­éŸ³æ—¶å‘ç”Ÿé”™è¯¯: {e}"
        print(error_message) # åœ¨æœåŠ¡å™¨æ—¥å¿—ä¸­æ‰“å°è¯¦ç»†é”™è¯¯ï¼Œæ–¹ä¾¿è°ƒè¯•

        # 2. å°†è¿™ä¸ªçº¯å‡€çš„å­—ç¬¦ä¸²æ”¾å…¥å“åº”ä½“ä¸­ã€‚
        #    è¿™æ ·å¯ä»¥ç¡®ä¿ jsonable_encoder ä¸ä¼šæ¥è§¦åˆ°åŸå§‹çš„ e å¯¹è±¡ã€‚
        return JSONResponse(
            content={"error": error_message},
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        )


@app.function(image=image, timeout=1800)
def run_gradio_app():
    """
    å¯åŠ¨ Gradio Web UI çš„å‡½æ•°ã€‚
    """
    import gradio as gr
    
    model_instance = VibeVoiceModel()

    def generate_speech_ui(text, temperature, top_p):
        if not text or not text.strip():
            gr.Warning("è¯·è¾“å…¥æ–‡æœ¬ï¼")
            return None
            
        print("æ”¶åˆ° Gradio è¯·æ±‚...")
        try:
            audio_bytes = model_instance.generate_speech.remote(
                text=text,
                temperature=temperature,
                top_p=top_p,
            )
            
            import tempfile
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                tmp_file.write(audio_bytes)
                return tmp_file.name
        except Exception as e:
            error_message = f"ç”Ÿæˆå¤±è´¥: {e}"
            print(error_message)
            gr.Error(error_message)
            return None

    with gr.Blocks(title="VibeVoice TTS", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ™ï¸ VibeVoice - é•¿å¯¹è¯è¯­éŸ³åˆæˆ")
        gr.Markdown("ä¸€ä¸ªé«˜è´¨é‡çš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ï¼Œæ”¯æŒå¤šè¯´è¯äººã€é•¿æ—¶é—´å¯¹è¯å’Œæƒ…æ„Ÿè¡¨è¾¾ã€‚")
        
        with gr.Row():
            with gr.Column(scale=2):
                text_input = gr.Textbox(
                    label="è¾“å…¥æ–‡æœ¬ (æ”¯æŒå¯¹è¯æ ¼å¼)",
                    placeholder="Alice: Hello, how are you?\nBob: I'm doing great, thanks for asking!",
                    lines=15,
                    value="Alice: Welcome to our podcast!\nBob: Thanks for having me, Alice.\nAlice: So tell us about your research."
                )
                with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                     with gr.Row():
                        temperature = gr.Slider(minimum=0.1, maximum=2.0, value=0.8, step=0.1, label="Temperature (éšæœºæ€§)")
                        top_p = gr.Slider(minimum=0.1, maximum=1.0, value=0.9, step=0.05, label="Top-p (æ ¸å¿ƒé‡‡æ ·)")
                
                generate_btn = gr.Button("ğŸµ ç”Ÿæˆè¯­éŸ³", variant="primary")

            with gr.Column(scale=1):
                audio_output = gr.Audio(label="ç”Ÿæˆçš„è¯­éŸ³", type="filepath")
                with gr.Accordion("ä½¿ç”¨è¯´æ˜å’Œç¤ºä¾‹", open=True):
                    gr.Markdown("""
                    ### ä½¿ç”¨è¯´æ˜:
                    1. **æ ¼å¼**: ä½¿ç”¨ `è¯´è¯äººå§“å: å¯¹è¯å†…å®¹` çš„æ ¼å¼ï¼Œæ¯å¥è¯å ä¸€è¡Œã€‚
                    2. **è¯­éŸ³æ–‡ä»¶**: æ‚¨æ— éœ€å‡†å¤‡ä»»ä½•æ–‡ä»¶ã€‚å¦‚æœ `demo/voices/` ç›®å½•ä¸‹æœ‰å¯¹åº”è¯´è¯äººåç§°çš„å°å†™ `.wav` æ–‡ä»¶ (ä¾‹å¦‚ `alice.wav`)ï¼Œç¨‹åºä¼šä½¿ç”¨å®ƒã€‚å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç¨‹åºä¼šè‡ªåŠ¨åˆ›å»ºä¸€æ®µé™éŸ³ä½œä¸ºæ›¿ä»£ã€‚
                    3. **è€å¿ƒ**: ç”Ÿæˆå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿æ–‡æœ¬ã€‚
                    
                    ### ç¤ºä¾‹:
                    ```
                    Alice: Welcome to our podcast!
                    Bob: Thanks for having me, Alice.
                    ```
                    """)
        
        generate_btn.click(
            fn=generate_speech_ui,
            inputs=[text_input, temperature, top_p],
            outputs=audio_output
        )

    demo.launch(server_name="0.0.0.0", server_port=8000, share=True)


@app.local_entrypoint()
def main():
    print("æ­£åœ¨å¯åŠ¨ Gradio Web ç•Œé¢... è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ¥æ„å»ºé•œåƒå’Œä¸‹è½½æ¨¡å‹ã€‚")
    run_gradio_app.remote()

