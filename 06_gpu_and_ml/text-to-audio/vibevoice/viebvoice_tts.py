import modal
import os
import re
from typing import List, Optional, Tuple, Dict

# --- é…ç½® ---
MODEL_REPO_ID = "vibevoice/VibeVoice-1.5B"
MODEL_CACHE_DIR = "/cache/models"
GPU_CONFIG = "A100-40GB"

# --- Modal App å®šä¹‰ ---
app = modal.App("vibevoice-tts-final-fixed")
model_cache = modal.Volume.from_name("vibevoice-cache", create_if_missing=True)

# --- ç¯å¢ƒé•œåƒå®šä¹‰ ---
image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install("git", "ffmpeg", "libsndfile1-dev")
    .pip_install(
        "torch",
        "torchaudio",
        "transformers",
        "gradio",
        "soundfile",
        "accelerate",
        "huggingface_hub",
        "packaging",
        "uv",
    )
    .run_commands(
        "git clone https://github.com/vibevoice-community/VibeVoice.git /app || true",
        "cd /app && uv pip install -e . --system",
    )
    .workdir("/app")
)


@app.cls(
    image=image,
    gpu=GPU_CONFIG,
    scaledown_window=300,
    timeout=3600,
    volumes={MODEL_CACHE_DIR: model_cache}
)
class VibeVoiceModel:
    """
    ä¸€ä¸ªå°è£…äº† VibeVoice æ¨¡å‹å’Œå¤„ç†å™¨çš„ç±»ï¼Œç”¨äºç”Ÿæˆè¯­éŸ³ã€‚
    """
    model: object = None
    processor: object = None
    model_path: str = None

    @modal.enter()
    def load_model_and_processor(self):
        """
        åœ¨å®¹å™¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ï¼Œå¹¶å°†å®ƒä»¬ç¼“å­˜åˆ°å®ä¾‹å˜é‡ä¸­ã€‚
        """
        import torch
        import sys
        import os

        try:
            from vibevoice.modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference as VVModel
            from vibevoice.processor.vibevoice_processor import VibeVoiceProcessor
        except ImportError as e:
            print(f"!!! å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
            print(f"    å½“å‰çš„ Python è·¯å¾„ (sys.path): {sys.path}")
            print(f"    /app ç›®å½•ä¸‹çš„å†…å®¹: {os.listdir('/app') if os.path.exists('/app') else 'ä¸å­˜åœ¨'}")
            if os.path.exists('/app/vibevoice'):
                print(f"    /app/vibevoice ç›®å½•ä¸‹çš„å†…å®¹: {os.listdir('/app/vibevoice')}")
            raise

        model_dir = os.path.join(MODEL_CACHE_DIR, MODEL_REPO_ID.split("/")[-1])
        if not os.path.exists(model_dir):
            print(f"æœ¬åœ°ç¼“å­˜ {model_dir} ä¸å­˜åœ¨ï¼Œå¼€å§‹ä» Hugging Face ä¸‹è½½...")
            from huggingface_hub import snapshot_download
            snapshot_download(
                repo_id=MODEL_REPO_ID,
                local_dir=model_dir,
                local_dir_use_symlinks=False,
            )
            model_cache.commit()
            print("æ¨¡å‹ä¸‹è½½å¹¶ç¼“å­˜æˆåŠŸã€‚")
        
        self.model_path = model_dir
        print(f"ä»è·¯å¾„ {self.model_path} åŠ è½½ VibeVoice æ¨¡å‹å’Œå¤„ç†å™¨...")

        self.processor = VibeVoiceProcessor.from_pretrained(self.model_path)
        print("å¤„ç†å™¨åŠ è½½æˆåŠŸã€‚")

        self.model = VVModel.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True 
        )
        self.model.eval()
        print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")

    @modal.method()
    def generate_speech(
        self,
        text: str,
        temperature: float = 0.8,
        top_p: float = 0.9,
    ) -> bytes:
        """
        æ ¹æ®è¾“å…¥æ–‡æœ¬ç”Ÿæˆè¯­éŸ³çš„æ ¸å¿ƒå‡½æ•°ã€‚
        åŸºäº VibeVoice å®˜æ–¹ Gradio demo çš„å®ç°æ–¹å¼ã€‚
        """
        import torch
        import tempfile
        import soundfile as sf
        import os
        import numpy as np

        if not self.model or not self.processor:
            raise RuntimeError("æ¨¡å‹æˆ–å¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ load_model_and_processor æ–¹æ³•ã€‚")

        try:
            # ç¡®ä¿è¾“å…¥æ–‡æœ¬ä¸ä¸ºç©º
            if not text or not text.strip():
                raise ValueError("è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©ºã€‚")
            
            print(f"æ”¶åˆ°è¾“å…¥æ–‡æœ¬: {repr(text)}")
            
            # è§£æå¯¹è¯æ–‡æœ¬å¹¶è½¬æ¢ä¸º VibeVoice æ ¼å¼
            lines = text.strip().split('\n')
            print(f"è§£æåˆ°çš„è¡Œæ•°: {len(lines)}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„å¯¹è¯æ ¼å¼
            valid_lines = [line for line in lines if ':' in line and line.strip()]
            if not valid_lines:
                raise ValueError("è¾“å…¥æ–‡æœ¬æ ¼å¼ä¸æ­£ç¡®ã€‚è¯·ä½¿ç”¨ 'è¯´è¯äºº: å†…å®¹' çš„æ ¼å¼ï¼Œæ¯å¥è¯å ä¸€è¡Œã€‚")

            # è½¬æ¢ä¸º VibeVoice æ ¼å¼ï¼šSpeaker 0:, Speaker 1: ç­‰
            formatted_script_lines = []
            speaker_mapping = {}  # è®°å½•è¯´è¯äººåç§°åˆ°æ•°å­—çš„æ˜ å°„
            speaker_counter = 0
            
            for line in valid_lines:
                line = line.strip()
                if not line:
                    continue
                    
                try:
                    speaker_name, content = line.split(':', 1)
                    speaker_name = speaker_name.strip()
                    content = content.strip()
                    
                    if content:  # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
                        # ä¸ºæ¯ä¸ªæ–°çš„è¯´è¯äººåˆ†é…æ•°å­—ID
                        if speaker_name not in speaker_mapping:
                            speaker_mapping[speaker_name] = speaker_counter
                            speaker_counter += 1
                        
                        speaker_id = speaker_mapping[speaker_name]
                        formatted_line = f"Speaker {speaker_id}: {content}"
                        formatted_script_lines.append(formatted_line)
                        print(f"è½¬æ¢: {line} -> {formatted_line}")
                except ValueError as e:
                    print(f"è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„è¡Œ: '{line}' - é”™è¯¯: {e}")
                    continue
            
            if not formatted_script_lines:
                raise ValueError("æ— æ³•ä»è¾“å…¥ä¸­è§£æå‡ºä»»ä½•æœ‰æ•ˆçš„å¯¹è¯è¡Œã€‚")

            formatted_script = '\n'.join(formatted_script_lines)
            print(f"æ ¼å¼åŒ–åçš„è„šæœ¬: {formatted_script}")
            
            # å‡†å¤‡éŸ³é¢‘æ ·æœ¬ - å®Œå…¨æŒ‰ç…§å®˜æ–¹ä»£ç çš„ read_audio å‡½æ•°
            sample_rate = 24000
            voice_samples = []
            
            def read_audio(audio_path: str, target_sr: int = 24000):
                """æŒ‰ç…§å®˜æ–¹ä»£ç çš„ read_audio å‡½æ•°å®ç°"""
                try:
                    import soundfile as sf
                    import librosa
                    wav, sr = sf.read(audio_path)
                    if len(wav.shape) > 1:
                        wav = np.mean(wav, axis=1)  # è½¬æ¢ä¸ºå•å£°é“
                    if sr != target_sr:
                        wav = librosa.resample(wav, orig_sr=sr, target_sr=target_sr)
                    return wav
                except Exception as e:
                    print(f"Error reading audio {audio_path}: {e}")
                    return np.array([])
            
            # å®šä¹‰å¯ç”¨çš„éŸ³é¢‘æ–‡ä»¶æ˜ å°„
            available_voices = {
                'Alice': 'en-Alice_woman.wav',
                'alice': 'en-Alice_woman.wav',
                'Carter': 'en-Carter_man.wav',
                'carter': 'en-Carter_man.wav',
                'Frank': 'en-Frank_man.wav',
                'frank': 'en-Frank_man.wav',
                'Maya': 'en-Maya_woman.wav',
                'maya': 'en-Maya_woman.wav',
                'Samuel': 'in-Samuel_man.wav',
                'samuel': 'in-Samuel_man.wav',
                'Anchen': 'zh-Anchen_man_bgm.wav',
                'anchen': 'zh-Anchen_man_bgm.wav',
                'Bowen': 'zh-Bowen_man.wav',
                'bowen': 'zh-Bowen_man.wav',
                'Xinran': 'zh-Xinran_woman.wav',
                'xinran': 'zh-Xinran_woman.wav',
            }
            
            for speaker_name in speaker_mapping.keys():
                # é¦–å…ˆå°è¯•ç›´æ¥æ˜ å°„åˆ°å¯ç”¨çš„éŸ³é¢‘æ–‡ä»¶
                mapped_filename = available_voices.get(speaker_name)
                
                if mapped_filename:
                    # ä½¿ç”¨æ˜ å°„çš„æ–‡ä»¶å
                    possible_paths = [
                        f"/app/demo/voices/{mapped_filename}",
                        f"demo/voices/{mapped_filename}",
                    ]
                    print(f"ä¸ºè¯´è¯äºº '{speaker_name}' ä½¿ç”¨æ˜ å°„æ–‡ä»¶: {mapped_filename}")
                else:
                    # å¦‚æœæ²¡æœ‰æ˜ å°„ï¼Œå°è¯•å¤šç§å¯èƒ½çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„æ ¼å¼
                    possible_paths = [
                        f"/app/demo/voices/{speaker_name}.wav",  # åŸå§‹æ ¼å¼
                        f"/app/demo/voices/{speaker_name.lower()}.wav",  # å°å†™æ ¼å¼
                        f"/app/demo/voices/{speaker_name.replace('_', '-')}.wav",  # ä¸‹åˆ’çº¿è½¬æ¨ªçº¿
                        f"/app/demo/voices/{speaker_name.lower().replace('_', '-')}.wav",  # å°å†™+ä¸‹åˆ’çº¿è½¬æ¨ªçº¿
                        f"demo/voices/{speaker_name}.wav",  # ç›¸å¯¹è·¯å¾„æ ¼å¼
                        f"demo/voices/{speaker_name.lower()}.wav",  # ç›¸å¯¹è·¯å¾„å°å†™æ ¼å¼
                    ]
                    print(f"ä¸ºè¯´è¯äºº '{speaker_name}' å°è¯•é€šç”¨è·¯å¾„åŒ¹é…")
                
                audio_path = None
                for path in possible_paths:
                    if os.path.exists(path):
                        audio_path = path
                        break
                
                if audio_path:
                    audio_data = read_audio(audio_path)
                    if len(audio_data) == 0:
                        print(f"Warning: æ— æ³•è¯»å– {speaker_name} çš„éŸ³é¢‘æ–‡ä»¶ã€‚ä½¿ç”¨é»˜è®¤é™éŸ³ã€‚")
                        # åˆ›å»º1ç§’çš„é™éŸ³
                        audio_data = np.zeros(sample_rate, dtype=np.float32)
                    voice_samples.append(audio_data)
                    print(f"åŠ è½½äº† {speaker_name} çš„éŸ³é¢‘æ–‡ä»¶ ({audio_path})ï¼Œé•¿åº¦: {len(audio_data)}")
                else:
                    print(f"Warning: æœªæ‰¾åˆ° {speaker_name} çš„éŸ³é¢‘æ–‡ä»¶ã€‚å°è¯•çš„è·¯å¾„: {possible_paths}")
                    # åˆ›å»º1ç§’çš„é™éŸ³
                    audio_data = np.zeros(sample_rate, dtype=np.float32)
                    voice_samples.append(audio_data)
            
            print(f"å‡†å¤‡äº† {len(voice_samples)} ä¸ªéŸ³é¢‘æ ·æœ¬")
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥éŸ³é¢‘æ ·æœ¬æ˜¯å¦ä¸ºç©º
            for i, sample in enumerate(voice_samples):
                if len(sample) == 0:
                    print(f"è­¦å‘Š: éŸ³é¢‘æ ·æœ¬ {i} ä¸ºç©º")
                elif np.all(sample == 0):
                    print(f"è­¦å‘Š: éŸ³é¢‘æ ·æœ¬ {i} å…¨ä¸ºé›¶ï¼ˆé™éŸ³ï¼‰")
                else:
                    print(f"éŸ³é¢‘æ ·æœ¬ {i} æ­£å¸¸ï¼Œé•¿åº¦: {len(sample)}, éé›¶å€¼æ•°é‡: {np.count_nonzero(sample)}")
            
            # ä½¿ç”¨å®˜æ–¹æ ¼å¼è°ƒç”¨ processor
            print("ä½¿ç”¨å®˜æ–¹æ ¼å¼è°ƒç”¨ processor...")
            inputs = self.processor(
                text=[formatted_script],  # æ³¨æ„ï¼šè¿™é‡Œæ˜¯åˆ—è¡¨
                voice_samples=[voice_samples],  # æ³¨æ„ï¼šè¿™é‡Œä¹Ÿæ˜¯åˆ—è¡¨
                padding=True,
                return_tensors="pt",
                return_attention_mask=True,
            )
            
            print("æ–‡æœ¬å·²æˆåŠŸå¤„ç†ä¸ºè¾“å…¥å¼ é‡ã€‚")
            print(f"Processor è¾“å‡ºé”®: {list(inputs.keys())}")
            
            # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡ - ä¸ä¿®æ”¹å½¢çŠ¶ï¼Œè®© processor è‡ªå·±å¤„ç†
            processed_inputs = {}
            for key, val in inputs.items():
                if torch.is_tensor(val):
                    processed_inputs[key] = val.to(self.model.device)
                    print(f"ç§»åŠ¨å¼ é‡åˆ°è®¾å¤‡: {key} -> {val.shape}")
                else:
                    processed_inputs[key] = val  # ä¿æŒéå¼ é‡å€¼ä¸å˜
                    print(f"ä¿æŒéå¼ é‡å€¼: {key} = {type(val)}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ input_ids
            if 'input_ids' not in processed_inputs:
                print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ° input_idsï¼Œæ£€æŸ¥ processor è¾“å‡º...")
                for key, val in processed_inputs.items():
                    print(f"  {key}: {type(val)} - {val if not torch.is_tensor(val) else val.shape}")
                # å°è¯•ä½¿ç”¨å…¶ä»–å¯èƒ½çš„é”®
                if 'input_token_ids' in processed_inputs:
                    processed_inputs['input_ids'] = processed_inputs['input_token_ids']
                    print("ä½¿ç”¨ input_token_ids ä½œä¸º input_ids")
                elif 'token_ids' in processed_inputs:
                    processed_inputs['input_ids'] = processed_inputs['token_ids']
                    print("ä½¿ç”¨ token_ids ä½œä¸º input_ids")
                else:
                    raise ValueError("æ— æ³•æ‰¾åˆ° input_ids æˆ–ç­‰æ•ˆçš„è¾“å…¥é”®")

            # ç”ŸæˆéŸ³é¢‘ - ä½¿ç”¨å®˜æ–¹å‚æ•°å¹¶æ·»åŠ æ›´å¤šæ§åˆ¶
            with torch.no_grad():
                print("è°ƒç”¨ model.generate å¼€å§‹ç”ŸæˆéŸ³é¢‘...")
                print(f"ä¼ é€’ç»™æ¨¡å‹çš„é”®: {list(processed_inputs.keys())}")
                
                # è®¡ç®—åˆé€‚çš„ max_new_tokens
                input_length = processed_inputs['input_ids'].shape[1]
                max_length = min(2048, input_length + 1000)  # é™åˆ¶æœ€å¤§é•¿åº¦
                print(f"è¾“å…¥é•¿åº¦: {input_length}, æœ€å¤§ç”Ÿæˆé•¿åº¦: {max_length}")
                
                audio_output = self.model.generate(
                    **processed_inputs,
                    max_new_tokens=max_length - input_length,  # æ˜ç¡®è®¾ç½®ç”Ÿæˆé•¿åº¦
                    cfg_scale=1.3,  # ä½¿ç”¨å®˜æ–¹é»˜è®¤å€¼
                    tokenizer=self.processor.tokenizer,
                    generation_config={
                        'do_sample': False,  # ä½¿ç”¨å®˜æ–¹è®¾ç½®
                        'max_length': max_length,
                        'min_length': input_length + 100,  # ç¡®ä¿ç”Ÿæˆè¶³å¤Ÿçš„é•¿åº¦
                    },
                    verbose=True,  # å¼€å¯è¯¦ç»†è¾“å‡º
                    refresh_negative=True,
                )
            
            print("éŸ³é¢‘ç”Ÿæˆå®Œæ¯•ã€‚")
            
            # å¤„ç† VibeVoiceGenerationOutput å¯¹è±¡
            if hasattr(audio_output, 'audio_values'):
                # å¦‚æœè¾“å‡ºæœ‰ audio_values å±æ€§
                waveform_tensor = audio_output.audio_values
            elif hasattr(audio_output, 'sequences'):
                # å¦‚æœè¾“å‡ºæœ‰ sequences å±æ€§
                waveform_tensor = audio_output.sequences
            elif isinstance(audio_output, tuple):
                # å¦‚æœæ˜¯å…ƒç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                waveform_tensor = audio_output[0]
            else:
                # ç›´æ¥ä½¿ç”¨è¾“å‡º
                waveform_tensor = audio_output
            
            # ç¡®ä¿æ˜¯å¼ é‡å¹¶è½¬æ¢ä¸º numpy
            if torch.is_tensor(waveform_tensor):
                waveform_np = waveform_tensor.cpu().to(torch.float32).numpy()
            else:
                # å¦‚æœä¸æ˜¯å¼ é‡ï¼Œå°è¯•ç›´æ¥è½¬æ¢
                waveform_np = np.array(waveform_tensor, dtype=np.float32)
            
            print(f"éŸ³é¢‘æ•°æ®å½¢çŠ¶: {waveform_np.shape}")
            
            # ç¡®ä¿æ˜¯2Dæ•°ç»„
            if len(waveform_np.shape) == 1:
                waveform_np = waveform_np.reshape(1, -1)
            elif len(waveform_np.shape) > 2:
                waveform_np = waveform_np.reshape(1, -1)

            # ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶å¹¶è¿”å›å­—èŠ‚
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                sf.write(tmp_file.name, waveform_np, samplerate=sample_rate)
                tmp_file.seek(0)
                audio_bytes = tmp_file.read()
            
            os.unlink(tmp_file.name)
            return audio_bytes

        except Exception as e:
            print(f"ç”Ÿæˆè¯­éŸ³æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            raise

# --- API å’Œ Gradio çš„ä»£ç ä¿æŒä¸å˜ ---

@app.function(image=image, min_containers=1)
@modal.fastapi_endpoint(method="POST")
def api(text_data: dict):
    """ä¸€ä¸ªç®€å•çš„ POST API å…¥å£ç‚¹ï¼Œç”¨äºç¨‹åºåŒ–è°ƒç”¨ã€‚"""
    text = text_data.get("text")
    if not text:
        from fastapi import status
        from fastapi.responses import JSONResponse
        return JSONResponse(
            content={"error": "ç¼ºå°‘ 'text' å­—æ®µ"},
            status_code=status.HTTP_400_BAD_REQUEST,
        )
    
    model = VibeVoiceModel()
    try:
        audio_bytes = model.generate_speech.remote(text)
        from fastapi.responses import Response
        return Response(content=audio_bytes, media_type="audio/wav")
    except Exception as e:
        from fastapi import status
        from fastapi.responses import JSONResponse
        return JSONResponse(
            content={"error": str(e)},
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        )


@app.function(image=image, timeout=1800)
def run_gradio_app():
    """
    å¯åŠ¨ Gradio Web UI çš„å‡½æ•°ã€‚
    """
    import gradio as gr
    
    model_instance = VibeVoiceModel()

    def generate_speech_ui(text, temperature, top_p):
        if not text or not text.strip():
            gr.Warning("è¯·è¾“å…¥æ–‡æœ¬ï¼")
            return None
            
        print("æ”¶åˆ° Gradio è¯·æ±‚...")
        print(f"è¾“å…¥æ–‡æœ¬: {repr(text)}")
        print(f"Temperature: {temperature}, Top-p: {top_p}")
        
        try:
            audio_bytes = model_instance.generate_speech.remote(
                text=text,
                temperature=temperature,
                top_p=top_p,
            )
            
            if audio_bytes is None:
                gr.Error("ç”Ÿæˆçš„éŸ³é¢‘ä¸ºç©º")
                return None
                
            import tempfile
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                tmp_file.write(audio_bytes)
                return tmp_file.name
        except Exception as e:
            error_message = f"ç”Ÿæˆå¤±è´¥: {e}"
            print(error_message)
            gr.Error(error_message)
            return None

    with gr.Blocks(title="VibeVoice TTS", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ™ï¸ VibeVoice - é•¿å¯¹è¯è¯­éŸ³åˆæˆ")
        gr.Markdown("ä¸€ä¸ªé«˜è´¨é‡çš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ï¼Œæ”¯æŒå¤šè¯´è¯äººã€é•¿æ—¶é—´å¯¹è¯å’Œæƒ…æ„Ÿè¡¨è¾¾ã€‚")
        
        with gr.Row():
            with gr.Column(scale=2):
                text_input = gr.Textbox(
                    label="è¾“å…¥æ–‡æœ¬ (æ”¯æŒå¯¹è¯æ ¼å¼)",
                    placeholder="Alice: Hello, how are you?\nCarter: I'm doing great, thanks for asking!",
                    lines=15,
                    value="Alice: Welcome to our podcast!\nCarter: Thanks for having me, Alice.\nAlice: So tell us about your research."
                )
                with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
                     with gr.Row():
                        temperature = gr.Slider(minimum=0.1, maximum=2.0, value=0.8, step=0.1, label="Temperature (éšæœºæ€§)")
                        top_p = gr.Slider(minimum=0.1, maximum=1.0, value=0.9, step=0.05, label="Top-p (æ ¸å¿ƒé‡‡æ ·)")
                
                generate_btn = gr.Button("ğŸµ ç”Ÿæˆè¯­éŸ³", variant="primary")

            with gr.Column(scale=1):
                audio_output = gr.Audio(label="ç”Ÿæˆçš„è¯­éŸ³", type="filepath")
                with gr.Accordion("ä½¿ç”¨è¯´æ˜å’Œç¤ºä¾‹", open=True):
                    gr.Markdown("""
                    ### ä½¿ç”¨è¯´æ˜:
                    1. **æ ¼å¼**: ä½¿ç”¨ `è¯´è¯äººå§“å: å¯¹è¯å†…å®¹` çš„æ ¼å¼ï¼Œæ¯å¥è¯å ä¸€è¡Œã€‚
                    2. **å¯ç”¨è¯´è¯äºº**: Alice, Carter, Frank, Maya, Samuel, Anchen, Bowen, Xinran
                    3. **è¯­éŸ³æ–‡ä»¶**: ç¨‹åºä¼šè‡ªåŠ¨ä½¿ç”¨é¢„ç½®çš„éŸ³é¢‘æ–‡ä»¶ï¼Œæ— éœ€é¢å¤–å‡†å¤‡ã€‚
                    4. **è€å¿ƒ**: ç”Ÿæˆå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿æ–‡æœ¬ã€‚
                    
                    ### ç¤ºä¾‹:
                    ```
                    Alice: Welcome to our podcast!
                    Carter: Thanks for having me, Alice.
                    ```
                    """)
        
        generate_btn.click(
            fn=generate_speech_ui,
            inputs=[text_input, temperature, top_p],
            outputs=audio_output
        )

    demo.launch(server_name="0.0.0.0", server_port=8000, share=True)


@app.local_entrypoint()
def main():
    print("æ­£åœ¨å¯åŠ¨ Gradio Web ç•Œé¢... è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ¥æ„å»ºé•œåƒå’Œä¸‹è½½æ¨¡å‹ã€‚")
    run_gradio_app.remote()


# æµ‹è¯•å‡½æ•° - å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œæ¥éªŒè¯æ–‡æœ¬è§£æé€»è¾‘
def test_text_parsing():
    """æµ‹è¯•æ–‡æœ¬è§£æé€»è¾‘"""
    test_text = """Alice: Welcome to our podcast!
Bob: Thanks for having me, Alice.
Alice: So tell us about your research."""
    
    lines = test_text.strip().split('\n')
    print(f"è§£æåˆ°çš„è¡Œæ•°: {len(lines)}")
    
    valid_lines = [line for line in lines if ':' in line and line.strip()]
    print(f"æœ‰æ•ˆè¡Œæ•°: {len(valid_lines)}")
    
    # æå–è¯´è¯äººå’Œæ–‡æœ¬
    speaker_names = []
    texts = []
    
    for line in valid_lines:
        try:
            speaker_name, content = line.split(':', 1)
            speaker_name = speaker_name.strip()
            content = content.strip()
            
            if content:  # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
                speaker_names.append(speaker_name)
                texts.append(content)
                print(f"æ·»åŠ å¯¹è¯: {speaker_name} -> {content}")
        except ValueError as e:
            print(f"è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„è¡Œ: '{line}' - é”™è¯¯: {e}")
            continue
    
    print(f"è¯´è¯äººåˆ—è¡¨: {speaker_names}")
    print(f"æœ€ç»ˆæ–‡æœ¬åˆ—è¡¨: {texts}")
    print(f"æ–‡æœ¬åˆ—è¡¨é•¿åº¦: {len(texts)}")
    print(f"è¯´è¯äººåˆ—è¡¨é•¿åº¦: {len(speaker_names)}")
    print("æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œå…ˆæµ‹è¯•æ–‡æœ¬è§£æ
    test_text_parsing()

