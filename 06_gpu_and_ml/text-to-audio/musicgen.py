# # Create your own music samples with MusicGen

# MusicGen is a popular open-source music-generation model family from Meta.
# In this example, we show you how you can run MusicGen models on Modal.

# We use [Audiocraft](https://github.com/facebookresearch/audiocraft), an inference library for audio models,
# including MusicGen and its kin, like AudioGen.

# ## Setting up the image and dependencies

from pathlib import Path
from uuid import uuid4

import modal

app = modal.App("example-musicgen")

MAX_SEGMENT_DURATION = 30

cache_dir = "/cache"
model_cache = modal.Volume.from_name(
    "audiocraft-model-cache", create_if_missing=True
)


def load_model(and_return=False):
    from audiocraft.models import MusicGen

    model_large = MusicGen.get_pretrained("facebook/musicgen-large")
    if and_return:
        return model_large


image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("git", "ffmpeg")
    .pip_install(
        "torch==2.1.0",  # version needed for audiocraft
        "pydub==0.25.1",
        "numpy<2",
        "git+https://github.com/facebookresearch/audiocraft.git@v1.3.0",
        "huggingface_hub[hf_transfer]==0.27.1",
    )
    .env({"HF_HUB_CACHE": cache_dir, "HF_HUB_ENABLE_HF_TRANSER": "1"})
    .run_function(load_model, volumes={cache_dir: model_cache})
)

with image.imports():
    import torch

# ## Defining the model generation

# We then write our model code within Modal's
# [`@app.cls`](/docs/reference/modal.App#cls) decorator, with the
# [`generate`] function processing the user input and generating audio as bytes that we can
# save to a file later.


@app.cls(gpu=modal.gpu.L40S(), image=image, volumes={cache_dir: model_cache})
class MusicGen:
    @modal.enter()
    def init(self):
        self.model = load_model(and_return=True)

    @modal.method()
    def generate(
        self,
        prompt: str,
        duration: int = 10,
        overlap: int = 10,
        format: str = "wav",
    ) -> bytes:
        f"""Generate a music clip based on the prompt.

        Clips longer than the MAX_SEGMENT_DURATION of {MAX_SEGMENT_DURATION}s
        are generated by clipping all but `overlap` seconds and running inference again."""
        context = None
        overlap = min(overlap, MAX_SEGMENT_DURATION - 1)
        remaining_duration = duration

        if remaining_duration < 0:
            return bytes()

        while remaining_duration > 0:
            # calculate duration of the next segment
            segment_duration = remaining_duration
            if context is not None:
                segment_duration += overlap

            segment_duration = min(segment_duration, MAX_SEGMENT_DURATION)

            # generate next segment
            self.model.set_generation_params(duration=segment_duration)
            next_segment = self._generate_next_segment(prompt, context, overlap)

            # update remaining duration
            remaining_duration -= (
                segment_duration
                if context is None
                else (segment_duration - overlap)
            )

            # combine with previous segments
            context = self._combine_segments(context, next_segment, overlap)

        output = context.detach().cpu().float()[0]

        return to_audio_bytes(
            output, self.model.sample_rate, strategy="loudness", format=format
        )

    def _generate_next_segment(self, prompt, context, overlap):
        """Generate the next audio segment, either fresh or as continuation of a context."""
        if context is None:
            return self.model.generate(descriptions=[prompt])
        else:
            last_chunk = context[:, :, -overlap * self.model.sample_rate :]
            return self.model.generate_continuation(
                last_chunk, self.model.sample_rate, descriptions=[prompt]
            )

    def _combine_segments(self, context, next_segment, overlap: int):
        """Combine context with next segment, handling overlap."""
        if context is None:
            return next_segment

        # Calculate where to trim the context (removing overlap)
        trim_samples = overlap * self.model.sample_rate
        context_trimmed = context[:, :, :-trim_samples]  # B, C, T

        return torch.cat([context_trimmed, next_segment], dim=2)


# We can call MusicGen inference from our local machine by running the code in the local entrypoint below.


@app.local_entrypoint()
def main(prompt: str = None, duration: int = 10, format: str = "wav"):
    if prompt is None:
        prompt = "Amapiano polka, klezmers, log drum bassline, 112 BPM"
    print(
        f"ðŸŽµ generating music from prompt '{prompt[:64] + ('...' if len(prompt) > 64 else '')}'"
    )

    audiocraft = MusicGen()
    clip = audiocraft.generate.remote(prompt, duration=duration, format=format)

    dir = Path("/tmp/audiocraft")
    dir.mkdir(exist_ok=True, parents=True)

    output_path = dir / f"{slugify(prompt)[:64]}.{format}"
    print(f"ðŸŽµ Saving to {output_path}")
    output_path.write_bytes(clip)


# You can execute the local entrypoint with:

# ``` shell
# modal run musicgen.py --prompt="metallica meets sabrina carpenter"
# ```

# ## A hosted Gradio interface

# With the Gradio library, we can create a simple web interface around our class in Python, then use Modal to host it for anyone to try out.
# To deploy your own, run

# ``` shell
# modal deploy musicgen.py
# ```

web_image = image.pip_install("fastapi[standard]==0.115.4", "gradio==4.44.1")


@app.function(
    image=web_image,
    # Gradio requires sticky sessions
    # so we limit the number of concurrent containers to 1
    # and allow it to scale to 1000 concurrent inputs
    concurrency_limit=1,
    allow_concurrent_inputs=1000,
)
@modal.asgi_app()
def ui():
    import gradio as gr
    from fastapi import FastAPI
    from gradio.routes import mount_gradio_app

    api = FastAPI()

    # Since this Gradio app is running from its own container,
    # allowing us to run the inference service via .remote() methods.
    model = MusicGen()

    temp_dir = Path("/dev/shm")

    async def generate_music(
        prompt: str, duration: int = 10, format: str = "wav"
    ):
        audio_bytes = await model.generate.remote.aio(prompt, duration, format)

        audio_file = f"{temp_dir}/{uuid4()}.{format}"
        audio_file.write_bytes(audio_bytes)

        return audio_file

    with gr.Blocks(theme="soft") as demo:
        gr.Markdown("# MusicGen")
        with gr.Row():
            with gr.Column():
                prompt = gr.Textbox(label="Prompt")
                duration = gr.Number(
                    label="Duration (seconds)", value=10, minimum=1, maximum=30
                )
                format = gr.Radio(["wav", "mp3"], label="Format", value="wav")
                btn = gr.Button("Generate")
            with gr.Column():
                clip_output = gr.Audio(label="Generated Music", autoplay=True)

        btn.click(
            generate_music,
            inputs=[prompt, duration, format],
            outputs=[clip_output],
        )

    return mount_gradio_app(app=api, blocks=demo, path="/")


def to_audio_bytes(wav, sample_rate: int, **kwargs) -> bytes:
    from audiocraft.data.audio import audio_write

    # audiocraft provides a nice utility for converting waveform tensors to audio,
    # but it saves to a file path. here, we create a file path that is actually
    # just backed by memory, instead of disk, to save on some latency

    shm = Path("/dev/shm")  # /dev/shm is a memory-backed filesystem
    stem_name = shm / str(uuid4())

    output_path = audio_write(stem_name, wav, sample_rate, **kwargs)

    return output_path.read_bytes()


def slugify(string):
    return (
        string.lower()
        .replace(" ", "-")
        .replace("/", "-")
        .replace("\\", "-")
        .replace(":", "-")
    )
