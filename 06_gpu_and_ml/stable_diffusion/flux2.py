# ---
# output-directory: "/tmp/flux2"
# args: ["--prompt", "A cinematic photo of a baby penguin"]
# ---

# # Run Flux2 (FLUX.2-dev) for high-quality image generation

# æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ Modal ä¸Šä½¿ç”¨ Black Forest Labs çš„ FLUX.2-dev æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾ç‰‡ã€‚
# FLUX.2 æ˜¯æ–°ä¸€ä»£çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæä¾›äº†æ›´å¥½çš„å›¾ç‰‡è´¨é‡ã€ä¸€è‡´æ€§å’Œæ§åˆ¶èƒ½åŠ›ã€‚

# ## è®¾ç½®é•œåƒå’Œä¾èµ–

import base64
import time
from io import BytesIO
from pathlib import Path
from pydantic import BaseModel

import modal
from modal import fastapi_endpoint
from fastapi.responses import Response, JSONResponse

# æˆ‘ä»¬ä½¿ç”¨å®Œæ•´çš„ CUDA å·¥å…·åŒ…æ¥æ„å»ºå®¹å™¨é•œåƒ

cuda_version = "12.4.0"  # ä¸åº”å¤§äºä¸»æœº CUDA ç‰ˆæœ¬
flavor = "devel"  # åŒ…å«å®Œæ•´çš„ CUDA å·¥å…·åŒ…
operating_sys = "ubuntu22.04"
tag = f"{cuda_version}-{flavor}-{operating_sys}"

cuda_dev_image = modal.Image.from_registry(
    f"nvidia/cuda:{tag}", add_python="3.11"
).entrypoint([])

# å®‰è£…ä¾èµ–ã€‚Flux2 éœ€è¦æœ€æ–°ç‰ˆæœ¬çš„ diffusers åº“æ¥æ”¯æŒ Flux2Pipeline
# æˆ‘ä»¬ä» GitHub ä¸»åˆ†æ”¯å®‰è£…ä»¥è·å¾—æœ€æ–°çš„ Flux2 æ”¯æŒ

flux2_image = (
    cuda_dev_image.apt_install(
        "git",
        "libglib2.0-0",
        "libsm6",
        "libxrender1",
        "libxext6",
        "ffmpeg",
        "libgl1",
    )
    .pip_install(
        "fastapi[standard]",
        "invisible_watermark==0.2.0",
        "transformers>=4.47.0",
        "huggingface_hub[hf_transfer]>=0.34.0",
        "hf-transfer",
        "accelerate>=0.33.0",
        "safetensors==0.4.4",
        "sentencepiece==0.2.0",
        "torch==2.5.0",
        "git+https://github.com/huggingface/diffusers.git@main",  # ä½¿ç”¨ä¸»åˆ†æ”¯ä»¥æ”¯æŒ Flux2
        "numpy<2",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1", "HF_HUB_CACHE": "/cache"})
)

# é…ç½® torch.compile ç¼“å­˜ä»¥åŠ å¿«åç»­å®¹å™¨çš„ç¼–è¯‘é€Ÿåº¦

flux2_image = flux2_image.env(
    {
        "TORCHINDUCTOR_CACHE_DIR": "/root/.inductor-cache",
        "TORCHINDUCTOR_FX_GRAPH_CACHE": "1",
    }
)


class ImageRequest(BaseModel):
    """å›¾ç‰‡ç”Ÿæˆè¯·æ±‚æ¨¡å‹"""
    api_key: str  # API å¯†é’¥ï¼Œå¿…å¡«
    prompt: str = "A cinematic photo of a baby penguin"
    input_images: list[str] | None = None  # å¯é€‰ï¼šbase64 ç¼–ç çš„è¾“å…¥å›¾ç‰‡åˆ—è¡¨ï¼Œç”¨äºå›¾ç”Ÿå›¾
    width: int = 1024  # å®½åº¦ï¼Œé»˜è®¤ 1024
    height: int = 1024  # é«˜åº¦ï¼Œé»˜è®¤ 1024
    num_inference_steps: int = 28  # æ¨ç†æ­¥æ•°ï¼Œé»˜è®¤ 28ï¼ˆå¯é€‰èŒƒå›´ï¼š28-50ï¼‰
    guidance_scale: float = 4.0  # å¼•å¯¼å¼ºåº¦ï¼Œé»˜è®¤ 4.0


# æ„å»º Modal Appï¼Œè®¾ç½®é»˜è®¤é•œåƒï¼Œå¹¶å¯¼å…¥ Flux2Pipeline

app = modal.App("example-flux2", image=flux2_image)

with flux2_image.imports():
    import torch
    from diffusers import Flux2Pipeline
    from diffusers.utils import load_image
    from PIL import Image

# ## å®šä¹‰å‚æ•°åŒ–çš„ Model æ¨ç†ç±»

# 1. ä½¿ç”¨ @modal.enter() è£…é¥°çš„æ–¹æ³•è¿è¡Œæ¨¡å‹è®¾ç½®ï¼ŒåŒ…æ‹¬åŠ è½½æƒé‡å¹¶ç§»è‡³ GPU
# 2. ä½¿ç”¨ @modal.method() è£…é¥°çš„æ–¹æ³•è¿è¡Œå®é™…æ¨ç†

# *æ³¨æ„: è®¿é—® Hugging Face ä¸Šçš„ FLUX.2-dev æ¨¡å‹éœ€è¦åŒæ„è®¸å¯åè®®ã€‚
# è¯·åœ¨ https://huggingface.co/black-forest-labs/FLUX.2-dev æ¥å—è®¸å¯åï¼Œ
# åˆ›å»ºåä¸º `huggingface-secret` çš„ Modal Secretã€‚*

MINUTES = 60  # ç§’
MODEL_ID = "black-forest-labs/FLUX.2-dev"
NUM_INFERENCE_STEPS = 50  # æ¨ç†æ­¥æ•°ï¼ˆå®˜æ–¹æ¨è50æ­¥ï¼Œ28æ­¥å¯ä½œä¸ºé€Ÿåº¦ä¸è´¨é‡çš„æŠ˜è¡·ï¼‰
GUIDANCE_SCALE = 4.0  # å¼•å¯¼å¼ºåº¦ï¼ˆFlux2 æ¨èå€¼ï¼‰


@app.cls(
    gpu="h200",  # ä½¿ç”¨ H200 GPU (141GB)ï¼Œä¸º FLUX.2-dev æä¾›æœ€å¤§å†…å­˜å’Œæœ€å¼ºæ€§èƒ½
    scaledown_window=20 * MINUTES,
    timeout=60 * MINUTES,  # ä¸ºç¼–è¯‘ç•™å‡ºå……è¶³çš„æ—¶é—´
    volumes={  # æ·»åŠ  Volumes ä»¥å­˜å‚¨å¯åºåˆ—åŒ–çš„ç¼–è¯‘å·¥ä»¶
        "/cache": modal.Volume.from_name("hf-hub-cache", create_if_missing=True),
        "/root/.nv": modal.Volume.from_name("nv-cache", create_if_missing=True),
        "/root/.triton": modal.Volume.from_name("triton-cache", create_if_missing=True),
        "/root/.inductor-cache": modal.Volume.from_name(
            "inductor-cache", create_if_missing=True
        ),
    },
    secrets=[modal.Secret.from_name("huggingface-secret")],
)
class Model:
    compile: bool = (  # æ˜¯å¦ä½¿ç”¨ torch.compile ä¼˜åŒ–
        modal.parameter(default=False)
    )

    @modal.enter()
    def enter(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print(f"ğŸ”¥ Loading FLUX.2-dev model from {MODEL_ID}...")
        pipe = Flux2Pipeline.from_pretrained(
            MODEL_ID, 
            torch_dtype=torch.bfloat16
        ).to("cuda")  # å°†æ¨¡å‹ç§»è‡³ GPU
        
        self.pipe = optimize(pipe, compile=self.compile)
        print("âœ… Model loaded successfully!")

    def _generate_image(
        self, 
        prompt: str,
        input_images: list[str] | None = None,
        width: int = 1024, 
        height: int = 1024,
        num_inference_steps: int = NUM_INFERENCE_STEPS,
        guidance_scale: float = GUIDANCE_SCALE,
    ) -> bytes:
        """å†…éƒ¨å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œæ”¯æŒæ–‡ç”Ÿå›¾å’Œå›¾ç”Ÿå›¾"""
        mode = "å›¾ç”Ÿå›¾" if input_images else "æ–‡ç”Ÿå›¾"
        print(f"ğŸ¨ æ¨¡å¼: {mode}")
        print(f"ğŸ¨ Generating image with size {width}x{height}, steps={num_inference_steps}, guidance={guidance_scale}...")
        print(f"ğŸ“ Prompt: {prompt}")
        
        # å¤„ç†è¾“å…¥å›¾ç‰‡ï¼ˆå¦‚æœæä¾›ï¼‰
        decoded_images = None
        if input_images:
            print(f"ğŸ–¼ï¸  Processing {len(input_images)} input image(s)...")
            decoded_images = []
            for i, base64_str in enumerate(input_images):
                try:
                    # è§£ç  base64 â†’ bytes â†’ PIL Imageï¼ˆå…¨ç¨‹åœ¨å†…å­˜ä¸­ï¼‰
                    image_data = base64.b64decode(base64_str)
                    pil_image = Image.open(BytesIO(image_data))
                    decoded_images.append(pil_image)
                    print(f"  âœ… Image {i+1}: {pil_image.size} ({pil_image.mode})")
                except Exception as e:
                    print(f"  âŒ Failed to decode image {i+1}: {e}")
                    raise ValueError(f"Invalid base64 image at index {i}")
        
        # å‡†å¤‡ pipeline å‚æ•°
        pipe_kwargs = {
            "prompt": prompt,
            "output_type": "pil",
            "num_inference_steps": num_inference_steps,
            "guidance_scale": guidance_scale,
            "width": width,
            "height": height,
            "generator": torch.Generator(device="cuda").manual_seed(42),
        }
        
        # å¦‚æœæœ‰è¾“å…¥å›¾ç‰‡ï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
        if decoded_images:
            pipe_kwargs["image"] = decoded_images
        
        # æ‰§è¡Œç”Ÿæˆ
        out = self.pipe(**pipe_kwargs).images[0]

        byte_stream = BytesIO()
        out.save(byte_stream, format="JPEG")
        return byte_stream.getvalue()

    @modal.method()
    def inference(
        self, 
        prompt: str,
        input_images: list[str] | None = None,
        width: int = 1024, 
        height: int = 1024,
        num_inference_steps: int = NUM_INFERENCE_STEPS,
        guidance_scale: float = GUIDANCE_SCALE,
    ) -> bytes:
        """ä¾› modal run å’Œ API è°ƒç”¨çš„æ¨ç†æ–¹æ³•"""
        return self._generate_image(prompt, input_images, width, height, num_inference_steps, guidance_scale)


# ## Web API ç«¯ç‚¹

# API å¯†é’¥ï¼ˆç¡¬ç¼–ç ï¼‰
API_KEY = "longlikun"

@app.function()
@modal.fastapi_endpoint(method="POST")
def web(request: ImageRequest):
    """å…¬å…± API ç«¯ç‚¹ï¼Œæ¥æ”¶ POST è¯·æ±‚ç”Ÿæˆå›¾ç‰‡ï¼ˆæ”¯æŒæ–‡ç”Ÿå›¾å’Œå›¾ç”Ÿå›¾ï¼‰"""
    # éªŒè¯ API å¯†é’¥
    if request.api_key != API_KEY:
        return JSONResponse(
            status_code=401,
            content={"error": "Invalid API key", "message": "è¯·æä¾›æ­£ç¡®çš„ API å¯†é’¥"}
        )
    
    image_bytes = Model().inference.remote(
        request.prompt,
        request.input_images,
        request.width,
        request.height,
        request.num_inference_steps,
        request.guidance_scale,
    )
    return Response(content=image_bytes, media_type="image/jpeg")


# ## å‘½ä»¤è¡Œå…¥å£ç‚¹

# ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œï¼š
# ```bash
# modal run flux2.py
# ```
#
# å¯é€‰å‚æ•°ï¼š
# - --prompt: æç¤ºè¯ï¼ˆé»˜è®¤ï¼š"A cinematic photo of a baby penguin"ï¼‰
# - --width: å›¾ç‰‡å®½åº¦ï¼ˆé»˜è®¤ï¼š1024ï¼‰
# - --height: å›¾ç‰‡é«˜åº¦ï¼ˆé»˜è®¤ï¼š1024ï¼‰
# - --num-inference-steps: æ¨ç†æ­¥æ•°ï¼ˆé»˜è®¤ï¼š28ï¼‰
# - --guidance-scale: å¼•å¯¼å¼ºåº¦ï¼ˆé»˜è®¤ï¼š4.0ï¼‰
# - --compile: ä½¿ç”¨ torch.compile ä¼˜åŒ–ï¼ˆé»˜è®¤ï¼šFalseï¼‰


@app.local_entrypoint()
def main(
    prompt: str = "A cinematic photo of a baby penguin playing with colorful blocks, soft lighting, shallow depth of field",
    width: int = 1024,
    height: int = 1024,
    num_inference_steps: int = NUM_INFERENCE_STEPS,
    guidance_scale: float = GUIDANCE_SCALE,
    compile: bool = False,
):
    """æœ¬åœ°å‘½ä»¤è¡Œå…¥å£ç‚¹"""
    print(f"ğŸš€ Starting Flux2 image generation...")
    print(f"ğŸ“ Prompt: {prompt}")
    print(f"ğŸ“ Size: {width}x{height}")
    print(f"ğŸ”¢ Steps: {num_inference_steps}, Guidance: {guidance_scale}")
    
    t0 = time.time()
    image_bytes = Model(compile=compile).inference.remote(
        prompt, width, height, num_inference_steps, guidance_scale
    )
    latency = time.time() - t0
    print(f"âš¡ Inference latency: {latency:.2f} seconds")

    output_path = Path("/tmp") / "flux2" / "output.jpg"
    output_path.parent.mkdir(exist_ok=True, parents=True)
    print(f"ğŸ’¾ Saving output to {output_path}")
    output_path.write_bytes(image_bytes)
    print(f"âœ… Image saved successfully!")


# ## ä½¿ç”¨ torch.compile åŠ é€Ÿ Flux2

# PyTorch 2 æ·»åŠ äº†ç¼–è¯‘å™¨æ¥ä¼˜åŒ– PyTorch æ‰§è¡ŒæœŸé—´åŠ¨æ€åˆ›å»ºçš„è®¡ç®—å›¾ã€‚
# è¿™æœ‰åŠ©äºç¼©å°ä¸ TensorRT å’Œ TensorFlow ç­‰é™æ€å›¾æ¡†æ¶çš„æ€§èƒ½å·®è·ã€‚

# ç¼–è¯‘åœ¨é¦–æ¬¡è¿­ä»£æ—¶å¯èƒ½éœ€è¦é•¿è¾¾ 20 åˆ†é’Ÿã€‚
# æˆ‘ä»¬ç¼“å­˜æ¥è‡ª nvccã€triton å’Œ inductor çš„ç¼–è¯‘è¾“å‡ºï¼Œ
# è¿™å¯ä»¥å°†ç¼–è¯‘æ—¶é—´å‡å°‘ä¸€ä¸ªæ•°é‡çº§ã€‚

# ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯ç”¨ç¼–è¯‘ï¼š
# ```bash
# modal run flux2.py --compile
# ```


def optimize(pipe, compile=False):
    """ä¼˜åŒ– pipeline ä»¥æé«˜æ¨ç†é€Ÿåº¦"""
    # èåˆ Transformer å’Œ VAE ä¸­çš„ QKV æŠ•å½±
    pipe.transformer.fuse_qkv_projections()
    pipe.vae.fuse_qkv_projections()

    # åˆ‡æ¢å†…å­˜å¸ƒå±€ä¸º Torch é¦–é€‰çš„ channels_last
    pipe.transformer.to(memory_format=torch.channels_last)
    pipe.vae.to(memory_format=torch.channels_last)

    if not compile:
        return pipe

    # è®¾ç½® torch compile æ ‡å¿—
    config = torch._inductor.config
    config.disable_progress = False  # æ˜¾ç¤ºè¿›åº¦æ¡
    config.conv_1x1_as_mm = True  # å°† 1x1 å·ç§¯è§†ä¸ºçŸ©é˜µä¹˜æ³•
    # è°ƒæ•´è‡ªåŠ¨è°ƒä¼˜ç®—æ³•
    config.coordinate_descent_tuning = True
    config.coordinate_descent_check_all_directions = True
    config.epilogue_fusion = False  # ä¸è¦å°†é€ç‚¹æ“ä½œèåˆåˆ°çŸ©é˜µä¹˜æ³•ä¸­

    # æ ‡è®°è®¡ç®—å¯†é›†å‹æ¨¡å—ï¼ˆTransformer å’Œ VAE decoderï¼‰è¿›è¡Œç¼–è¯‘
    pipe.transformer = torch.compile(
        pipe.transformer, mode="max-autotune", fullgraph=True
    )
    pipe.vae.decode = torch.compile(
        pipe.vae.decode, mode="max-autotune", fullgraph=True
    )

    # è§¦å‘ torch ç¼–è¯‘
    print("ğŸ”¦ Running torch compilation (may take up to 20 minutes)...")

    pipe(
        "dummy prompt to trigger torch compilation",
        output_type="pil",
        num_inference_steps=NUM_INFERENCE_STEPS,
        guidance_scale=GUIDANCE_SCALE,
    ).images[0]

    print("ğŸ”¦ Finished torch compilation")

    return pipe
