# ---
# cmd: ["modal", "run", "06_gpu_and_ml/reinforcement-learning/trl-grpo.py::train"]
# ---

# # Run GRPO on Modal using TRL

# This example demonstrates how to run [GRPO](https://arxiv.org/pdf/2402.03300) on Modal using the TRL [GRPO trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer)
# GRPO is a reinforcement learning algorithm introduced by DeepSeek, and was used to train DeepSeek R1.
# TRL is a reinforcement learning training library by Huggingface.

# First we perform the imports and then define the app.
from __future__ import annotations
from typing import Iterable, Sequence, Tuple

import modal
import os
import subprocess

app: modal.App = modal.App("grpo-trl-example")

# We define an image where we install the TRL library.
# We also install vLLM for the next part of this example. We also use Weights & Biases for logging.
image: modal.Image = modal.Image.debian_slim().pip_install(
    "trl[vllm]", "datasets==3.5.1", "wandb==0.17.6"
)

# We import the necessary libraries needed in the context of the image.
with image.imports():
    from datasets import Dataset, load_dataset
    from trl import GRPOConfig, GRPOTrainer

# We also a define a [Modal Volume](https://modal.com/docs/guide/volumes#volumes) for storing model checkpoints.
MODELS_DIR = "/models"
checkpoints_volume: modal.Volume = modal.Volume.from_name(
    "grpo-trl-example-checkpoints", create_if_missing=True
)

# ## Defining the reward function

# In this example, we use the [OpenCoder-LLM/opc-sft-stage2](https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2) dataset to train a model to solve coding problems.

# In reinforcement learning, we define a reward function for the model. Since we are evaluating code that is generated by
# a model, we use [Modal Sandboxes](https://modal.com/docs/guide/sandbox) to evaluate the code securely.


# For each completion from the model and a test case to test the completion, we define a simple reward function.
# The function returns 1 if there are no errors, and 0 otherwise. You might want to adjust this reward function
# as the model is unlikely to learn well with this function.

@app.function()
def compute_reward(completion: str, testcase: Sequence[str]) -> int:
    sb, score = None, 0
    try:
        sb: modal.Sandbox = modal.Sandbox.create(app=app)
    except:
        raise Exception("Unable to create sandbox")
    
    code_to_execute: str = get_generated_code_and_test_cases(
        completion, testcase
    )

    try:
        p = sb.exec("python", "-c", code_to_execute, timeout=60)
        p.wait()
        return_code = p.returncode
        if return_code == 0:
            score = 1
    except:
        print("Sandbox execution failed")
    finally:
        if sb:
            sb.terminate()
        return score 
        

# We write a function that constructs a program from the model completion. This is determined based on the format of the data
# The completions are supposed to follow the format <TEXT>```python <CODE>```
# The test cases are a list of assert statements.
# More details [here](https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2/viewer/educational_instruct/train?views%5B%5D=educational_instruct&row=0)
def get_generated_code_and_test_cases(completion: str, testcase: Sequence[str]) -> str:
    if "```python" in completion:
        # Find the start and end of the code block
        start_idx: int = completion.find("```python") + len("```python")
        end_idx: int = completion.find("```", start_idx)
        if end_idx != -1:
            code: str = completion[start_idx:end_idx].strip()
        else:
            code: str = completion[start_idx:].strip()
    else:
        code: str = completion.strip()

    test_cases: str = "\n".join(testcase)
    full_code: str = f"{code}\n\n{test_cases}"
    return full_code


# Finally, we define the function that is passed into the GRPOTrainer, which takes in a list of completions
# Custom reward functions must conform to a [specific signature](https://huggingface.co/docs/trl/main/en/grpo_trainer#using-a-custom-reward-function)
def reward_helper_function(
    completions: Sequence[str],
    testcases: Sequence[Sequence[str]],
    **kwargs: object
) -> Iterable[int]:
    return compute_reward.starmap(zip(completions, testcases))

# ## Kicking off a training run

# Preprocess the data, preparing the columns that `GRPOTrainer` expects
def start_grpo_trainer(use_vllm=False, vllm_mode=None):
    dataset: Dataset = load_dataset(
        "OpenCoder-LLM/opc-sft-stage2", "educational_instruct", split="train"
    )
    dataset = dataset.rename_column(
        "instruction", "prompt"
    )  # needed for the GRPO trainer
    dataset = dataset.rename_column("testcase", "testcases")
    dataset = dataset.select(range(128)) # To simplify testing. Remove for production use cases.
    training_args: GRPOConfig = GRPOConfig(
        output_dir = MODELS_DIR, report_to = "wandb", use_vllm = use_vllm, vllm_mode = vllm_mode, max_steps = 5, save_steps = 1, # To simplify testing. Remove for production use cases. 
    )
    trainer = GRPOTrainer(
        model="Qwen/Qwen2-0.5B-Instruct",
        reward_funcs=reward_helper_function,
        args=training_args,
        train_dataset=dataset,
    )
    trainer.train() 


# We use Weights & Biases for logging, hence we use a [Modal Secret](https://modal.com/docs/guide/secrets#secrets) with wandb credentials
@app.function(
    image=image,
    gpu="H100!",
    timeout=60 * 60 * 24,  # 24 hours
    secrets=[modal.Secret.from_name("wandb-secret")],
    volumes = {
        "/models": checkpoints_volume
    }
)
def train() -> None:
    start_grpo_trainer()    


# To run: `modal run --detach trl-grpo.py::train``

# ## Speeding up training with vLLM

# vLLM can be used either in server mode (run vLLM server on separate gpu) or colocate mode (within the training process)
# In server mode, vLLM runs in a separate process (and using separate GPUs) and communicates with the trainer via HTTP. 
# This is ideal if you have dedicated GPUs for inference. More details [here.](https://huggingface.co/docs/trl/main/en/grpo_trainer#-option-1-server-mode)
# Here, we use 2 GPUs. We run the GRPOTrainer on 1 of them, and the vLLM process on another.
@app.function(
    image=image,
    gpu="H100!:2",
    timeout=60 * 60 * 24,  # 24 hours
    secrets=[modal.Secret.from_name("wandb-secret")],
    volumes = {
        MODELS_DIR: checkpoints_volume
    }    
)
def train_vllm_server_mode() -> None:
    env_copy = os.environ.copy()
    env_copy["CUDA_VISIBLE_DEVICES"] = "0"

    # Start vllm-serve in the background
    subprocess.Popen(
        ["trl", "vllm-serve", "--model", "Qwen/Qwen2-0.5B-Instruct"],
        env=env_copy,
    )
    os.environ["CUDA_VISIBLE_DEVICES"] = "1" # Run on separate GPU
    start_grpo_trainer(use_vllm=True, vllm_mode="server")  

# You can execute this using `modal run --detach trl-grpo.py::train_vllm_server_mode`

# In colocate mode, vLLM runs inside the trainer process and shares GPU memory with the training model. 
# This avoids launching a separate server and can improve GPU utilization, but may lead to memory contention on the training GPUs.
# More details (here.)[https://huggingface.co/docs/trl/main/en/grpo_trainer#-option-2-colocate-mode]

@app.function(
    image=image,
    gpu="H100!",
    timeout=60 * 60 * 24,  # 24 hours
    secrets=[modal.Secret.from_name("wandb-secret")],
    volumes = {
        "/models": checkpoints_volume
    }    
)
def train_vllm_colocate_mode() -> None:
    # Environment variables to set for colocate mode on single GPU.
    os.environ["RANK"] = "0"
    os.environ["LOCAL_RANK"] = "0"
    os.environ["WORLD_SIZE"] = "1"
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12355"
    start_grpo_trainer(use_vllm=True, vllm_mode="colocate")  

# You can execute this using `modal run --detach trl-grpo.py::train_vllm_colocate_mode`
