# ---
# lambda-test:false
# ---

# # Run GRPO on Modal using TRL

# This example demonstrates how to run [GRPO](https://arxiv.org/pdf/2402.03300) on Modal using the TRL [GRPO trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer)
# GRPO is a reinforcement learning algorithm introduced by DeepSeek, and was used to train DeepSeek R1.
# TRL is a reinforcement learning training library by Huggingface.

# First we perform the imports and then define the app.

import subprocess

import modal

app = modal.App("grpo-trl-example")

# We define an image where we install the TRL library.
# We also install vLLM for the next part of this example. We also use WANDB for logging.
image = modal.Image.debian_slim().pip_install(
    "trl[vllm]==0.19.0", "datasets==3.5.1", "wandb==0.17.6"
)

# We define some constants

# ## Defining the reward function

# In this example, we use the OpenCoder-LLM/opc-sft-stage2 dataset to train a model to solve coding problems.

# In reinforcement learning, we define a reward function for the model. Since we are evaluating code that is generated by
# a model, we use [Modal Sandboxes](https://modal.com/docs/guide/sandbox) to evaluate the code securely.


# For each completion from the model and a test case to test the completion, we define a simple reward function.
# The function returns 1 if there are no errors, and 0 otherwise.
@app.function()
def compute_reward(completion, testcase):
    sb = modal.Sandbox.create(app=app)
    code_to_execute = get_generated_code_and_test_cases(
        completion, testcase
    )  # defined below
    p = sb.exec("python", "-c", code_to_execute)
    p.wait()
    sb.terminate()
    print("ASDSAD", p.returncode)
    if p.returncode == 0:
        return 1
    else:
        return 0


# We write a function that constructs a program from the model completion. This is determined based on the format of the data
# The completions are supposed to follow the format <TEXT>```python <CODE>```
# The test cases are a list of assert statements.
# More details [here](https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2/viewer/educational_instruct/train?views%5B%5D=educational_instruct&row=0)
def get_generated_code_and_test_cases(completion, testcase):
    if "```python" in completion:
        # Find the start and end of the code block
        start_idx = completion.find("```python") + len("```python")
        end_idx = completion.find("```", start_idx)
        if end_idx != -1:
            code = completion[start_idx:end_idx].strip()
        else:
            code = completion[start_idx:].strip()
    else:
        code = completion.strip()

    test_cases = "\n".join(testcase)
    full_code = f"{code}\n\n{test_cases}"
    return full_code


# Finally, we define the function that is passed into the GRPOTrainer, which takes in a list of completions
# Custom reward functions must conform to a [specific signature](https://huggingface.co/docs/trl/main/en/grpo_trainer#using-a-custom-reward-function)
def reward_helper_function(prompts, completions, testcases, **kwargs):
    return compute_reward.starmap(zip(completions, testcases))


# ## Kicking off a training run

# We import the necessary libraries
with image.imports():
    from datasets import load_dataset
    from trl import GRPOConfig, GRPOTrainer


# Preprocess the data, preparing the columns that `GRPOTrainer` expects
def load_and_preprocess_data():
    dataset = load_dataset(
        "OpenCoder-LLM/opc-sft-stage2", "educational_instruct", split="train"
    )
    dataset = dataset.rename_column(
        "instruction", "prompt"
    )  # needed for the GRPO trainer
    dataset = dataset.rename_column("testcase", "testcases")
    return dataset


# We use WANDB for logging, hence we use a [Modal Secret](https://modal.com/docs/guide/secrets#secrets) with wandb credentials
@app.function(
    image=image,
    gpu="H100",
    timeout=86400,
    secrets=[modal.Secret.from_name("wandb-secret")],
)
def train():
    dataset = load_and_preprocess_data()
    training_args = GRPOConfig(
        output_dir="Qwen2-0.5B-GRPO", report_to="wandb"
    )  # comment if don't want to use wandb

    trainer = GRPOTrainer(
        model="Qwen/Qwen2-0.5B-Instruct",
        reward_funcs=reward_helper_function,
        args=training_args,
        train_dataset=dataset,
    )
    trainer.train()


# To run: `modal run --detach trl-grpo.py::train``

# ## Speeding up training with vLLM

# vLLM can be used either in server mode (run vLLM server on separate gpu) or colocate mode (within the training process)

@app.function(
    image=image,
    gpu="H100:2",
    timeout=86400,
    secrets=[modal.Secret.from_name("wandb-secret")],
)
def train_vllm_server_mode():
    dataset = load_and_preprocess_data()
    subprocess.run(
        ["trl", "vllm-serve", "--model", "Qwen/Qwen2-0.5B-Instruct"],
        env={"CUDA_VISIBLE_DEVICES": "0"}, # Run on separate GPU
    )
    training_args = GRPOConfig(output_dir="Qwen2-0.5B-GRPO")
    trainer = GRPOTrainer(
        model="Qwen/Qwen2-0.5B-Instruct",
        reward_funcs=reward_helper_function,
        args=training_args,
        train_dataset=dataset,
        use_vllm=True,
        vllm_mode="server",
    )
    trainer.train()

# You can execute this using `modal run --detach trl-grpo.py::train_vllm_server_mode`

@app.function(
    image=image,
    gpu="H100",
    timeout=86400,
    secrets=[modal.Secret.from_name("wandb-secret")],
)
def train_vllm_colocate_mode():
    dataset = load_and_preprocess_data()
    training_args = GRPOConfig(output_dir="Qwen2-0.5B-GRPO")
    trainer = GRPOTrainer(
        model="Qwen/Qwen2-0.5B-Instruct",
        reward_funcs=reward_helper_function,
        args=training_args,
        train_dataset=dataset,
        use_vllm=True,
        vllm_mode="colocate",
    )
    trainer.train()

# You can execute this using `modal run --detach trl-grpo.py::train_vllm_colocate_mode`
