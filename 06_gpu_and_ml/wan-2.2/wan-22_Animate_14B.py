# ---
# output-directory: "/tmp/wan22-animate"
# ---

# # ä½¿ç”¨ Modal éƒ¨ç½² Wan2.2-Animate-14B è§’è‰²åŠ¨ç”»å’Œæ›¿æ¢æ¨¡å‹

# Wan2.2-Animate-14B æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§’è‰²åŠ¨ç”»å’Œæ›¿æ¢æ¨¡å‹ã€‚
# 
# åŠŸèƒ½ï¼š
# 1. Animation æ¨¡å¼ï¼šè®©é™æ€è§’è‰²å›¾ç‰‡æŒ‰ç…§å‚è€ƒè§†é¢‘çš„åŠ¨ä½œåŠ¨èµ·æ¥
# 2. Replacement æ¨¡å¼ï¼šå°†è§†é¢‘ä¸­çš„è§’è‰²æ›¿æ¢æˆæŒ‡å®šè§’è‰²
#
# âš ï¸  æ³¨æ„ï¼šæ­¤æ¨¡å‹éœ€è¦ä½¿ç”¨ GitHub åŸå§‹ä»£ç ï¼Œä¸æ”¯æŒ Diffusers

# æ¨¡å‹ä¸»é¡µ: https://huggingface.co/Wan-AI/Wan2.2-Animate-14B
# GitHub: https://github.com/Wan-Video/Wan2.2
# é¡¹ç›®é¡µé¢: https://humanaigc.github.io/wan-animate

from pathlib import Path
from typing import Optional, Literal
import modal

# 1. å®šä¹‰å®¹å™¨é•œåƒï¼šå…‹éš† GitHub ä»“åº“å¹¶å®‰è£…ä¾èµ–
image = (
    modal.Image.from_registry(
        "nvidia/cuda:12.1.1-devel-ubuntu22.04",
        add_python="3.11",
    )
    .apt_install("git", "ffmpeg", "libgl1-mesa-glx", "libglib2.0-0")
    .pip_install("pip==24.0")
    # å…ˆå®‰è£… PyTorchï¼ˆflash_attn ç¼–è¯‘éœ€è¦ï¼‰
    .pip_install(
        "torch>=2.4.0",
        "torchvision",
        extra_index_url="https://download.pytorch.org/whl/cu121",
    )
    .run_commands(
        # å…‹éš† Wan2.2 GitHub ä»“åº“
        "cd /root && git clone https://github.com/Wan-Video/Wan2.2.git",
    )
    # å•ç‹¬å®‰è£…é™¤äº† flash_attn ä¹‹å¤–çš„ä¾èµ–ï¼ˆflash_attn ç¼–è¯‘å¤ªæ…¢ä¸”å®¹æ˜“å¤±è´¥ï¼‰
    .pip_install(
        "transformers>=4.44.0",
        "diffusers>=0.30.0",
        "accelerate>=0.29.0",
        "sentencepiece",
        "protobuf",
        "ftfy",
        "Pillow>=10.2.0",
        "numpy",
        "opencv-python",
        "imageio[ffmpeg]",
        "einops",
        "omegaconf",
        "safetensors",
        "huggingface-hub",
        # é¢„å¤„ç†éœ€è¦çš„åº“
        "mediapipe",
        "insightface",
        "onnxruntime-gpu",
    )
    # è·³è¿‡ flash_attnï¼Œå®ƒç¼–è¯‘å¤ªæ…¢ä¸”ä¸æ˜¯å¿…éœ€çš„
    # å¦‚æœçœŸçš„éœ€è¦ï¼Œå¯ä»¥ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬æˆ–åœ¨è¿è¡Œæ—¶ä½¿ç”¨ scaled_dot_product_attention
)

# å®šä¹‰æ¨¡å‹è·¯å¾„å’Œç¼“å­˜
MODEL_NAME = "Wan-AI/Wan2.2-Animate-14B"
CACHE_DIR = Path("/cache")
REPO_DIR = Path("/root/Wan2.2")

# åˆ›å»ºæŒä¹…åŒ–å­˜å‚¨å·
cache_volume = modal.Volume.from_name("hf-hub-cache-wan22-animate", create_if_missing=True)
volumes = {CACHE_DIR: cache_volume}

# HuggingFace API å¯†é’¥
secrets = [modal.Secret.from_name("huggingface-secret")]

app = modal.App("example-wan22-animate-character-animation")


@app.cls(
    image=image,
    gpu="H100",  # Animate éœ€è¦å¤§æ˜¾å­˜
    volumes=volumes,
    secrets=secrets,
    timeout=3600,  # 1å°æ—¶è¶…æ—¶ï¼ˆé¢„å¤„ç† + æ¨ç†éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
    scaledown_window=300,
)
class Model:
    @modal.enter()
    def enter(self):
        """
        å®¹å™¨å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡ï¼šä¸‹è½½æ¨¡å‹æƒé‡
        """
        import os
        import subprocess
        
        print(f"æ­£åœ¨ä¸‹è½½æ¨¡å‹: {MODEL_NAME}")
        
        # è®¾ç½®å·¥ä½œç›®å½•
        os.chdir(REPO_DIR)
        
        # ä¸‹è½½æ¨¡å‹æƒé‡åˆ°ç¼“å­˜ç›®å½•
        model_path = CACHE_DIR / "Wan2.2-Animate-14B"
        if not model_path.exists():
            print("é¦–æ¬¡è¿è¡Œï¼Œæ­£åœ¨ä¸‹è½½æ¨¡å‹æƒé‡...")
            subprocess.run([
                "huggingface-cli", "download",
                MODEL_NAME,
                "--local-dir", str(model_path)
            ], check=True)
        else:
            print("æ¨¡å‹å·²ç¼“å­˜ï¼Œè·³è¿‡ä¸‹è½½")
        
        self.model_path = model_path
        print("æ¨¡å‹å‡†å¤‡å®Œæˆï¼")

    @modal.method()
    def preprocess(
        self,
        video_bytes: bytes,
        image_bytes: bytes,
        mode: Literal["animation", "replacement"] = "animation",
        resolution_width: int = 1280,
        resolution_height: int = 720,
    ) -> bytes:
        """
        é¢„å¤„ç†æ­¥éª¤ï¼šå¤„ç†è¾“å…¥è§†é¢‘å’Œå‚è€ƒå›¾ç‰‡
        
        å‚æ•°:
        - video_bytes: è¾“å…¥è§†é¢‘çš„å­—èŠ‚æµ
        - image_bytes: å‚è€ƒè§’è‰²å›¾ç‰‡çš„å­—èŠ‚æµ
        - mode: "animation" æˆ– "replacement"
        - resolution_width: è§†é¢‘å®½åº¦ï¼ˆé»˜è®¤ 1280ï¼‰
        - resolution_height: è§†é¢‘é«˜åº¦ï¼ˆé»˜è®¤ 720ï¼‰
        
        è¿”å›:
        - é¢„å¤„ç†ç»“æœçš„æ‰“åŒ…å­—èŠ‚æµ
        """
        import os
        import subprocess
        import tarfile
        from io import BytesIO
        
        print(f"å¼€å§‹é¢„å¤„ç† - æ¨¡å¼: {mode}")
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = Path("/tmp/animate_input")
        temp_dir.mkdir(exist_ok=True, parents=True)
        
        # ä¿å­˜è¾“å…¥æ–‡ä»¶
        video_path = temp_dir / "video.mp4"
        image_path = temp_dir / "image.jpeg"
        video_path.write_bytes(video_bytes)
        image_path.write_bytes(image_bytes)
        
        # é¢„å¤„ç†è¾“å‡ºç›®å½•
        output_dir = temp_dir / "process_results"
        output_dir.mkdir(exist_ok=True)
        
        # æ„å»ºé¢„å¤„ç†å‘½ä»¤
        os.chdir(REPO_DIR)
        
        cmd = [
            "python", "./wan/modules/animate/preprocess/preprocess_data.py",
            "--ckpt_path", str(self.model_path / "process_checkpoint"),
            "--video_path", str(video_path),
            "--refer_path", str(image_path),
            "--save_path", str(output_dir),
            "--resolution_area", str(resolution_width), str(resolution_height),
        ]
        
        # æ ¹æ®æ¨¡å¼æ·»åŠ ç‰¹å®šå‚æ•°
        if mode == "animation":
            cmd.extend(["--retarget_flag", "--use_flux"])
        else:  # replacement
            cmd.extend([
                "--iterations", "3",
                "--k", "7",
                "--w_len", "1",
                "--h_len", "1",
                "--replace_flag"
            ])
        
        print(f"è¿è¡Œé¢„å¤„ç†å‘½ä»¤: {' '.join(cmd)}")
        subprocess.run(cmd, check=True)
        
        # æ‰“åŒ…é¢„å¤„ç†ç»“æœ
        print("æ‰“åŒ…é¢„å¤„ç†ç»“æœ...")
        tar_buffer = BytesIO()
        with tarfile.open(fileobj=tar_buffer, mode='w:gz') as tar:
            tar.add(output_dir, arcname='process_results')
        
        result_bytes = tar_buffer.getvalue()
        print(f"é¢„å¤„ç†å®Œæˆï¼Œç»“æœå¤§å°: {len(result_bytes) / 1024 / 1024:.2f} MB")
        
        return result_bytes

    @modal.method()
    def generate(
        self,
        preprocessed_bytes: bytes,
        mode: Literal["animation", "replacement"] = "animation",
        use_multi_gpu: bool = False,
    ) -> bytes:
        """
        ç”Ÿæˆæ­¥éª¤ï¼šä½¿ç”¨é¢„å¤„ç†ç»“æœç”Ÿæˆæœ€ç»ˆè§†é¢‘
        
        å‚æ•°:
        - preprocessed_bytes: é¢„å¤„ç†ç»“æœçš„æ‰“åŒ…å­—èŠ‚æµ
        - mode: "animation" æˆ– "replacement"
        - use_multi_gpu: æ˜¯å¦ä½¿ç”¨å¤šGPUï¼ˆå½“å‰å•GPUéƒ¨ç½²è®¾ä¸º Falseï¼‰
        
        è¿”å›:
        - ç”Ÿæˆçš„è§†é¢‘å­—èŠ‚æµ
        """
        import os
        import subprocess
        import tarfile
        from io import BytesIO
        
        print(f"å¼€å§‹ç”Ÿæˆè§†é¢‘ - æ¨¡å¼: {mode}")
        
        # è§£å‹é¢„å¤„ç†ç»“æœ
        temp_dir = Path("/tmp/animate_generate")
        temp_dir.mkdir(exist_ok=True, parents=True)
        
        tar_buffer = BytesIO(preprocessed_bytes)
        with tarfile.open(fileobj=tar_buffer, mode='r:gz') as tar:
            tar.extractall(temp_dir)
        
        process_results_dir = temp_dir / "process_results"
        
        # æ„å»ºç”Ÿæˆå‘½ä»¤
        os.chdir(REPO_DIR)
        
        cmd = [
            "python", "generate.py",
            "--task", "animate-14B",
            "--ckpt_dir", str(self.model_path),
            "--src_root_path", str(process_results_dir),
            "--refert_num", "1",
        ]
        
        # æ·»åŠ æ¨¡å¼ç‰¹å®šå‚æ•°
        if mode == "replacement":
            cmd.extend(["--replace_flag", "--use_relighting_lora"])
        
        print(f"è¿è¡Œç”Ÿæˆå‘½ä»¤: {' '.join(cmd)}")
        subprocess.run(cmd, check=True)
        
        # æŸ¥æ‰¾ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶
        output_video = None
        for video_file in process_results_dir.glob("**/*.mp4"):
            if "output" in video_file.name.lower() or "result" in video_file.name.lower():
                output_video = video_file
                break
        
        if not output_video:
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª mp4 æ–‡ä»¶
            output_video = next(process_results_dir.glob("**/*.mp4"), None)
        
        if not output_video:
            raise FileNotFoundError("æœªæ‰¾åˆ°ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶")
        
        print(f"æ‰¾åˆ°ç”Ÿæˆè§†é¢‘: {output_video}")
        video_bytes = output_video.read_bytes()
        print(f"è§†é¢‘å¤§å°: {len(video_bytes) / 1024 / 1024:.2f} MB")
        
        return video_bytes


@app.local_entrypoint()
def main(
    video_path: str,
    image_path: str,
    mode: str = "animation",
    output_path: str = "/tmp/wan22-animate/output.mp4",
    resolution_width: int = 1280,
    resolution_height: int = 720,
):
    """
    æœ¬åœ°å…¥å£å‡½æ•°ï¼šå®Œæ•´çš„è§’è‰²åŠ¨ç”»/æ›¿æ¢æµç¨‹
    
    ç”¨æ³•ç¤ºä¾‹:
    
    1. Animation æ¨¡å¼ï¼ˆè®©é™æ€è§’è‰²åŠ¨èµ·æ¥ï¼‰:
       modal run wan22_animate_deploy.py \
           --video-path ./dance_video.mp4 \
           --image-path ./character.jpg \
           --mode animation
    
    2. Replacement æ¨¡å¼ï¼ˆæ›¿æ¢è§†é¢‘ä¸­çš„è§’è‰²ï¼‰:
       modal run wan22_animate_deploy.py \
           --video-path ./original_video.mp4 \
           --image-path ./new_character.jpg \
           --mode replacement
    """
    video_path = Path(video_path)
    image_path = Path(image_path)
    output_path = Path(output_path)
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not video_path.exists():
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è§†é¢‘æ–‡ä»¶ {video_path}")
        return
    if not image_path.exists():
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶ {image_path}")
        return
    
    # éªŒè¯æ¨¡å¼
    if mode not in ["animation", "replacement"]:
        print(f"é”™è¯¯ï¼šæ¨¡å¼å¿…é¡»æ˜¯ 'animation' æˆ– 'replacement'")
        return
    
    print(f"ğŸ­ æ¨¡å¼: {mode.upper()}")
    print(f"ğŸ¬ è¾“å…¥è§†é¢‘: {video_path}")
    print(f"ğŸ–¼ï¸  è§’è‰²å›¾ç‰‡: {image_path}")
    print(f"ğŸ“ åˆ†è¾¨ç‡: {resolution_width}x{resolution_height}")
    
    # è¯»å–è¾“å…¥æ–‡ä»¶
    print("\nğŸ“¤ ä¸Šä¼ è¾“å…¥æ–‡ä»¶...")
    video_bytes = video_path.read_bytes()
    image_bytes = image_path.read_bytes()
    
    # æ­¥éª¤ 1: é¢„å¤„ç†
    print("\nğŸ”„ æ­¥éª¤ 1/2: é¢„å¤„ç†è§†é¢‘å’Œå›¾ç‰‡ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
    model = Model()
    preprocessed_bytes = model.preprocess.remote(
        video_bytes=video_bytes,
        image_bytes=image_bytes,
        mode=mode,
        resolution_width=resolution_width,
        resolution_height=resolution_height,
    )
    print(f"âœ… é¢„å¤„ç†å®Œæˆ")
    
    # æ­¥éª¤ 2: ç”Ÿæˆè§†é¢‘
    print("\nğŸ¨ æ­¥éª¤ 2/2: ç”Ÿæˆæœ€ç»ˆè§†é¢‘ï¼ˆè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰...")
    video_result_bytes = model.generate.remote(
        preprocessed_bytes=preprocessed_bytes,
        mode=mode,
    )
    
    # ä¿å­˜ç»“æœ
    output_path.parent.mkdir(exist_ok=True, parents=True)
    output_path.write_bytes(video_result_bytes)
    print(f"\nâœ… å®Œæˆï¼è§†é¢‘å·²ä¿å­˜åˆ°: {output_path}")
    print(f"ğŸ’¾ æ–‡ä»¶å¤§å°: {len(video_result_bytes) / 1024 / 1024:.2f} MB")