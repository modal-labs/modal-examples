# ---
# output-directory: "/tmp/wan22-video"
# ---

# # ä½¿ç”¨ Modal éƒ¨ç½² Wan2.2-Animate-14B è§†é¢‘ç”Ÿæˆæ¨¡å‹

# åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨äº‘ç«¯GPUä¸Šè¿è¡Œé˜¿é‡Œçš„ Wan2.2-Animate-14B æ¨¡å‹ã€‚
# è¿™æ˜¯ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆè§†é¢‘ (T2V) åŠ¨ç”»æ¨¡å‹ï¼Œ
# å¯ä»¥ç”Ÿæˆ 720P@24fps çš„é«˜è´¨é‡è§†é¢‘ã€‚

# æ¨¡å‹ä¸»é¡µ: https://huggingface.co/Wan-AI/Wan2.2-Animate-14B
# GitHub: https://github.com/Wan-Video/Wan2.2

from io import BytesIO
from pathlib import Path
from typing import Optional

import modal
from fastapi import File, Form, UploadFile
from fastapi.responses import Response


# 1. å®šä¹‰å®¹å™¨é•œåƒï¼šå®‰è£…æ‰€æœ‰å¿…è¦çš„åº“
image = (
    modal.Image.from_registry(
        "nvidia/cuda:12.1.1-devel-ubuntu22.04",
        add_python="3.11",
    )
    .apt_install("git", "ffmpeg")  # ffmpeg ç”¨äºè§†é¢‘å¤„ç†
    .pip_install(
        "torch>=2.4.0",
        "torchvision",
        "Pillow>=10.2.0",
        "huggingface-hub>=0.22.0",
        "accelerate>=0.29.0",
        "sentencepiece",  # T5 æ¨¡å‹éœ€è¦
        "protobuf",
        "ftfy",  # WanPipeline æ–‡æœ¬å¤„ç†éœ€è¦
        "fastapi",
        "python-multipart",
        "numpy",
        "decord",
        "opencv-python",
        "imageio[ffmpeg]",  # è§†é¢‘å¯¼å‡º
        extra_index_url="https://download.pytorch.org/whl/cu121",
    )
    .run_commands(
        "git clone https://github.com/Wan-Video/Wan2.2.git /opt/Wan2.2",
        "cd /opt/Wan2.2 && pip install -r requirements.txt"
    )
)

# å®šä¹‰ Wan2.2 ä»“åº“è·¯å¾„
WAN_PATH = Path("/opt/Wan2.2")

# åˆ›å»ºä¸€ä¸ªæŒä¹…åŒ–çš„å­˜å‚¨å·æ¥ç¼“å­˜æ¨¡å‹
cache_volume = modal.Volume.from_name("hf-hub-cache-wan22", create_if_missing=True)
volumes = {Path("/cache"): cache_volume}

# ä»Modalå¹³å°å®‰å…¨åœ°è·å–HuggingFaceçš„APIå¯†é’¥
secrets = [modal.Secret.from_name("huggingface-secret")]

app = modal.App("example-wan22-animate")

@app.cls(
    image=image,
    gpu="H100",  # Wan2.2-Animate-14B å»ºè®®ä½¿ç”¨ 80GB H100 GPU
    volumes=volumes,
    secrets=secrets,
    timeout=3600,  # 60åˆ†é’Ÿè¶…æ—¶ï¼Œè§†é¢‘ç”Ÿæˆéœ€è¦è¾ƒé•¿æ—¶é—´
    scaledown_window=300,  # ä¿®å¤ï¼šä½¿ç”¨æ–°çš„å‚æ•°å
)
class Model:
    @modal.enter()
    def enter(self):
        """
        å®¹å™¨å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡:ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹åˆ°GPUã€‚
        """
        import torch
        import sys
        sys.path.append(str(WAN_PATH))
        from wan2_2.models import WanModel
        from wan2_2.pipeline import WanAnimatePipeline

        print("åŠ è½½ Wan2.2 Animate 14B æ¨¡å‹...")
        torch.backends.cuda.matmul.allow_tf32 = True
        self.device = "cuda"
        self.dtype = torch.bfloat16

        self.pipe = WanAnimatePipeline.from_pretrained(
            WAN_PATH / "models/Wan2.2-Animate-14B",
            torch_dtype=self.dtype,
            device=self.device
        )
        self.pipe.to(self.device)
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")

    @modal.method()
    def generate_video(
        self,
        prompt: str,
        image_bytes: Optional[bytes] = None,
        height: int = 704,
        width: int = 1280,
        num_frames: int = 121,
        num_inference_steps: int = 50,
        guidance_scale: float = 5.0,
        seed: int = 42,
    ) -> bytes:
        """
        æ ¸å¿ƒçš„è§†é¢‘ç”Ÿæˆå‡½æ•°ã€‚
        
        å‚æ•°:
        - prompt: æ–‡æœ¬æç¤ºè¯
        - image_bytes: å¯é€‰çš„è¾“å…¥å›¾ç‰‡ï¼ˆå¦‚æœæä¾›åˆ™ä¸º I2Vï¼Œå¦åˆ™ä¸º T2Vï¼‰
        - height: è§†é¢‘é«˜åº¦ï¼ˆé»˜è®¤ 704ï¼‰
        - width: è§†é¢‘å®½åº¦ï¼ˆé»˜è®¤ 1280ï¼‰
        - num_frames: å¸§æ•°ï¼ˆé»˜è®¤ 121ï¼Œçº¦5ç§’@24fpsï¼‰
        - num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆé»˜è®¤ 50ï¼‰
        - guidance_scale: å¼•å¯¼å¼ºåº¦ï¼ˆé»˜è®¤ 5.0ï¼‰
        - seed: éšæœºç§å­
        
        è¿”å›:
        - ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶ï¼ˆMP4æ ¼å¼ï¼‰çš„å­—èŠ‚æµ
        """
        import torch
        from PIL import Image
        from diffusers.utils import export_to_video
        import numpy as np

        print(f"æ”¶åˆ°æ–°çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡")
        print(f"æç¤ºè¯: '{prompt}'")
        print(f"æ¨¡å¼: {'å›¾ç‰‡ç”Ÿæˆè§†é¢‘ (I2V)' if image_bytes else 'æ–‡æœ¬ç”Ÿæˆè§†é¢‘ (T2V)'}")

        # è´Ÿå‘æç¤ºè¯ï¼ˆä¸­æ–‡ + è‹±æ–‡ï¼‰
        negative_prompt = (
            "è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œ"
            "æ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œ"
            "ç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œ"
            "æ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°"
        )

        # å¦‚æœæä¾›äº†å›¾ç‰‡ï¼Œåˆ™è¿›è¡Œ I2V ç”Ÿæˆ
        image = None
        if image_bytes:
            image = Image.open(BytesIO(image_bytes)).convert("RGB")
            print(f"è¾“å…¥å›¾ç‰‡åŸå§‹å°ºå¯¸: {image.size}")
            
            # å¯¹äº I2Vï¼Œéœ€è¦æ ¹æ®å›¾ç‰‡æ¯”ä¾‹è°ƒæ•´å°ºå¯¸
            # TI2V-5B çš„ size å‚æ•°è¡¨ç¤ºé¢ç§¯ï¼Œå®½é«˜æ¯”è·Ÿéšè¾“å…¥å›¾ç‰‡
            max_area = height * width  # ä½¿ç”¨ä¼ å…¥çš„ height å’Œ width è®¡ç®—é¢ç§¯
            aspect_ratio = image.height / image.width
            
            # è®¡ç®—åˆé€‚çš„å°ºå¯¸ï¼ˆå¿…é¡»æ˜¯ 32 çš„å€æ•°ï¼Œå› ä¸º patch_size=2, scale_factor=16ï¼‰
            mod_value = 32
            calc_height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value
            calc_width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value
            
            # è°ƒæ•´å›¾ç‰‡å¤§å°
            image = image.resize((calc_width, calc_height))
            print(f"è°ƒæ•´åå›¾ç‰‡å°ºå¯¸: {image.size}")
            
            # ä½¿ç”¨è°ƒæ•´åçš„å°ºå¯¸
            height = calc_height
            width = calc_width

        print(f"è§†é¢‘åˆ†è¾¨ç‡: {width}x{height}, å¸§æ•°: {num_frames}")

        # è®¾ç½®ç”Ÿæˆå™¨
        generator = torch.Generator(device=self.device).manual_seed(seed)

        # ç”Ÿæˆè§†é¢‘
        print("å¼€å§‹ç”Ÿæˆè§†é¢‘...")
        output = self.pipe(
            prompt=prompt,
            image=image,  # None è¡¨ç¤º T2Vï¼Œæœ‰å€¼è¡¨ç¤º I2V
            negative_prompt=negative_prompt,
            height=height,
            width=width,
            num_frames=num_frames,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
            generator=generator,
        ).frames[0]

        print(f"è§†é¢‘ç”Ÿæˆå®Œæˆï¼å¸§æ•°: {len(output)}")

        # å¯¼å‡ºä¸º MP4
        print("æ­£åœ¨å¯¼å‡ºè§†é¢‘...")
        video_path = "/tmp/output_video.mp4"
        export_to_video(output, video_path, fps=24)

        # è¯»å–è§†é¢‘æ–‡ä»¶ä¸ºå­—èŠ‚æµ
        video_bytes = Path(video_path).read_bytes()
        print(f"è§†é¢‘æ–‡ä»¶å¤§å°: {len(video_bytes) / 1024 / 1024:.2f} MB")

        return video_bytes


@app.function(image=image, timeout=1800)
@modal.fastapi_endpoint(method="POST")  # ä¿®å¤ï¼šä½¿ç”¨æ–°çš„è£…é¥°å™¨åç§°
async def generate_video_api(
    prompt: str = Form(...),
    image: Optional[UploadFile] = File(None),
    height: int = Form(704),
    width: int = Form(1280),
    num_frames: int = Form(121),
    num_inference_steps: int = Form(50),
    guidance_scale: float = Form(5.0),
    seed: int = Form(42),
):
    """
    Web API ç«¯ç‚¹ï¼Œç”¨äºé€šè¿‡ HTTP POST è¯·æ±‚ç”Ÿæˆè§†é¢‘ã€‚
    
    ä½¿ç”¨ multipart/form-data æ ¼å¼ï¼š
    - prompt: æ–‡æœ¬æç¤ºè¯ï¼ˆå¿…å¡«ï¼‰
    - image: å¯é€‰çš„è¾“å…¥å›¾ç‰‡æ–‡ä»¶
    - height: è§†é¢‘é«˜åº¦ï¼ˆé»˜è®¤ 704ï¼‰
    - width: è§†é¢‘å®½åº¦ï¼ˆé»˜è®¤ 1280ï¼‰
    - num_frames: å¸§æ•°ï¼ˆé»˜è®¤ 121ï¼‰
    - num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆé»˜è®¤ 50ï¼‰
    - guidance_scale: å¼•å¯¼å¼ºåº¦ï¼ˆé»˜è®¤ 5.0ï¼‰
    - seed: éšæœºç§å­ï¼ˆé»˜è®¤ 42ï¼‰
    """
    print(f"æ”¶åˆ°æ¥è‡ª Web çš„è¯·æ±‚ï¼Œæç¤ºè¯: '{prompt}'")

    # è¯»å–ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
    image_bytes = None
    if image:
        image_bytes = await image.read()
        print(f"æ”¶åˆ°è¾“å…¥å›¾ç‰‡ï¼Œå¤§å°: {len(image_bytes)} bytes")

    # è¿œç¨‹è°ƒç”¨æ ¸å¿ƒç”Ÿæˆå‡½æ•°
    video_bytes = Model().generate_video.remote(
        prompt=prompt,
        image_bytes=image_bytes,
        height=height,
        width=width,
        num_frames=num_frames,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        seed=seed,
    )

    # è¿”å›ç”Ÿæˆçš„è§†é¢‘
    return Response(content=video_bytes, media_type="video/mp4")


@app.local_entrypoint()
def main(
    prompt: str = "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage",
    image_path: Optional[str] = None,
    output_path: str = "/tmp/wan22-video/output.mp4",
    height: int = 704,
    width: int = 1280,
    num_frames: int = 121,
    num_inference_steps: int = 50,
    guidance_scale: float = 5.0,
    seed: int = 42,
):
    """
    æœ¬åœ°å…¥å£å‡½æ•°ï¼šè°ƒç”¨äº‘ç«¯æ¨¡å‹ç”Ÿæˆè§†é¢‘ï¼Œä¿å­˜ç»“æœã€‚
    
    ç”¨æ³•ç¤ºä¾‹:
    1. æ–‡æœ¬ç”Ÿæˆè§†é¢‘ (T2V):
       modal run wan22_deploy.py --prompt "ä¸€åªå¯çˆ±çš„ç†ŠçŒ«åœ¨ç«¹æ—é‡Œç©è€"
    
    2. å›¾ç‰‡ç”Ÿæˆè§†é¢‘ (I2V):
       modal run wan22_deploy.py --prompt "è¿™åªçŒ«åœ¨æµ·æ»©ä¸Šå†²æµª" --image-path ./cat.jpg
    """
    output_video_path = Path(output_path)

    # è¯»å–è¾“å…¥å›¾ç‰‡ï¼ˆå¦‚æœæä¾›ï¼‰
    image_bytes = None
    if image_path:
        input_image_path = Path(image_path)
        if not input_image_path.exists():
            print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥å›¾ç‰‡ {input_image_path}")
            return
        print(f"ğŸ¬ æ­£åœ¨è¯»å–è¾“å…¥å›¾ç‰‡: {input_image_path}")
        image_bytes = input_image_path.read_bytes()

    mode = "å›¾ç‰‡ç”Ÿæˆè§†é¢‘ (I2V)" if image_bytes else "æ–‡æœ¬ç”Ÿæˆè§†é¢‘ (T2V)"
    print(f"ğŸ¬ æ¨¡å¼: {mode}")
    print(f"ğŸ¬ æç¤ºè¯: '{prompt}'")
    print(f"ğŸ¬ åˆ†è¾¨ç‡: {width}x{height}")
    print(f"ğŸ¬ å¸§æ•°: {num_frames} ({num_frames/24:.1f}ç§’ @ 24fps)")
    print(f"ğŸ¬ æ­£åœ¨äº‘ç«¯ç”Ÿæˆè§†é¢‘ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")

    # è°ƒç”¨è¿œç¨‹ç”Ÿæˆå‡½æ•°
    video_bytes = Model().generate_video.remote(
        prompt=prompt,
        image_bytes=image_bytes,
        height=height,
        width=width,
        num_frames=num_frames,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        seed=seed,
    )

    # ä¿å­˜è§†é¢‘
    output_video_path.parent.mkdir(exist_ok=True, parents=True)
    print(f"ğŸ¬ æ­£åœ¨ä¿å­˜è§†é¢‘åˆ°: {output_video_path}")
    output_video_path.write_bytes(video_bytes)
    print(f"âœ… å®Œæˆï¼è§†é¢‘å·²ä¿å­˜åˆ°: {output_video_path}")