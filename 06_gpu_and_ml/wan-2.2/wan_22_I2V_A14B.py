# ---
# output-directory: "/tmp/wan22-i2v"
# ---

# # ä½¿ç”¨ Modal éƒ¨ç½² Wan2.2-I2V-A14B å›¾ç‰‡ç”Ÿæˆè§†é¢‘æ¨¡å‹

# Wan2.2-I2V-A14B æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå›¾ç‰‡ç”Ÿæˆè§†é¢‘çš„ MoE æ¶æ„æ¨¡å‹ã€‚
# å®ƒé‡‡ç”¨åŒä¸“å®¶è®¾è®¡ï¼šé«˜å™ªå£°ä¸“å®¶å¤„ç†æ—©æœŸé˜¶æ®µï¼Œä½å™ªå£°ä¸“å®¶å¤„ç†ç»†èŠ‚ç²¾ä¿®ã€‚
# æ”¯æŒ 480P å’Œ 720P åˆ†è¾¨ç‡ï¼Œæ€»å‚æ•° 27Bï¼ˆæ¯æ­¥æ¿€æ´» 14Bï¼‰ã€‚

# æ¨¡å‹ä¸»é¡µ: https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers
# GitHub: https://github.com/Wan-Video/Wan2.2

from io import BytesIO
from pathlib import Path
from typing import Optional

import modal
from fastapi import File, Form, UploadFile
from fastapi.responses import Response


# 1. å®šä¹‰å®¹å™¨é•œåƒï¼šå®‰è£…æ‰€æœ‰å¿…è¦çš„åº“
image = (
    modal.Image.from_registry(
        "nvidia/cuda:12.1.1-devel-ubuntu22.04",
        add_python="3.11",
    )
    .apt_install("git", "ffmpeg")
    .pip_install(
        "torch>=2.4.0",
        "torchvision",
        "transformers==4.44.2",  # ç¨³å®šç‰ˆæœ¬
        "diffusers>=0.34.0",  # æ”¯æŒ Wan2.2
        "Pillow>=10.2.0",
        "huggingface-hub>=0.22.0",
        "accelerate>=0.29.0",
        "sentencepiece",
        "protobuf",
        "ftfy",
        "fastapi",
        "python-multipart",
        "numpy",
        "imageio[ffmpeg]",
        extra_index_url="https://download.pytorch.org/whl/cu121",
    )
)

# å®šä¹‰æ¨¡å‹åç§°å’Œç¼“å­˜è·¯å¾„
MODEL_NAME = "Wan-AI/Wan2.2-I2V-A14B-Diffusers"
CACHE_DIR = Path("/cache")

# åˆ›å»ºæŒä¹…åŒ–å­˜å‚¨å·
cache_volume = modal.Volume.from_name("hf-hub-cache-wan22-i2v", create_if_missing=True)
volumes = {CACHE_DIR: cache_volume}

# HuggingFace API å¯†é’¥
secrets = [modal.Secret.from_name("huggingface-secret")]

app = modal.App("example-wan22-i2v-video-generation")

@app.cls(
    image=image,
    gpu="H200",  # I2V-A14B éœ€è¦ 80GB æ˜¾å­˜
    volumes=volumes,
    secrets=secrets,
    timeout=1800,
    scaledown_window=300,
)
class Model:
    @modal.enter()
    def enter(self):
        """
        å®¹å™¨å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡ï¼šä¸‹è½½å¹¶åŠ è½½æ¨¡å‹åˆ°GPUã€‚
        """
        import torch
        from diffusers import WanImageToVideoPipeline

        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_NAME}")
        self.device = "cuda"
        self.dtype = torch.bfloat16

        # åŠ è½½ Image-to-Video Pipeline
        print("åŠ è½½ Wan I2V Pipeline...")
        try:
            self.pipe = WanImageToVideoPipeline.from_pretrained(
                MODEL_NAME,
                torch_dtype=self.dtype,
                cache_dir=CACHE_DIR,
            )
        except ValueError as e:
            print(f"å¸¸è§„åŠ è½½å¤±è´¥ï¼Œå°è¯•å…¼å®¹æ¨¡å¼: {e}")
            self.pipe = WanImageToVideoPipeline.from_pretrained(
                MODEL_NAME,
                torch_dtype=self.dtype,
                cache_dir=CACHE_DIR,
                low_cpu_mem_usage=False,
                ignore_mismatched_sizes=True,
            )
        
        self.pipe.to(self.device)
        
        # è·å– VAE ç¼©æ”¾å› å­å’Œ patch size
        self.vae_scale_factor = self.pipe.vae_scale_factor_spatial
        self.patch_size = self.pipe.transformer.config.patch_size[1]

        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print(f"VAE ç©ºé—´ç¼©æ”¾å› å­: {self.vae_scale_factor}")
        print(f"Patch å¤§å°: {self.patch_size}")

    def _calculate_video_size(self, image_width: int, image_height: int, max_area: int = 480 * 832):
        """
        æ ¹æ®è¾“å…¥å›¾ç‰‡å°ºå¯¸å’Œæœ€å¤§é¢ç§¯è®¡ç®—è§†é¢‘å°ºå¯¸ã€‚
        ä¿æŒåŸå›¾æ¯”ä¾‹ï¼Œå¹¶ç¡®ä¿å°ºå¯¸æ˜¯ vae_scale_factor * patch_size çš„å€æ•°ã€‚
        """
        import numpy as np
        
        aspect_ratio = image_height / image_width
        mod_value = self.vae_scale_factor * self.patch_size
        
        # æ ¹æ®æœ€å¤§é¢ç§¯å’Œå®½é«˜æ¯”è®¡ç®—å°ºå¯¸
        height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value
        width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value
        
        return width, height

    @modal.method()
    def generate_video(
        self,
        image_bytes: bytes,
        prompt: str,
        max_area: int = 480 * 832,  # 480P: 480*832, 720P: 720*1280
        num_frames: int = 81,  # é»˜è®¤ 81 å¸§ï¼ˆçº¦ 5 ç§’ @ 16fpsï¼‰
        num_inference_steps: int = 40,
        guidance_scale: float = 3.5,
        seed: int = 0,
    ) -> bytes:
        """
        å›¾ç‰‡ç”Ÿæˆè§†é¢‘çš„æ ¸å¿ƒå‡½æ•°ã€‚
        
        å‚æ•°:
        - image_bytes: è¾“å…¥å›¾ç‰‡çš„å­—èŠ‚æµï¼ˆå¿…å¡«ï¼‰
        - prompt: æ–‡æœ¬æç¤ºè¯ï¼ˆæè¿°è§†é¢‘å†…å®¹ï¼‰
        - max_area: æœ€å¤§é¢ç§¯ï¼ˆ480*832=480P, 720*1280=720Pï¼‰
        - num_frames: å¸§æ•°ï¼ˆé»˜è®¤ 81ï¼Œçº¦ 5 ç§’ @ 16fpsï¼‰
        - num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆé»˜è®¤ 40ï¼‰
        - guidance_scale: å¼•å¯¼å¼ºåº¦ï¼ˆé»˜è®¤ 3.5ï¼‰
        - seed: éšæœºç§å­
        
        è¿”å›:
        - ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶ï¼ˆMP4æ ¼å¼ï¼‰çš„å­—èŠ‚æµ
        """
        import torch
        from PIL import Image
        from diffusers.utils import export_to_video
        import numpy as np

        print(f"æ”¶åˆ°å›¾ç‰‡ç”Ÿæˆè§†é¢‘ä»»åŠ¡")
        print(f"æç¤ºè¯: '{prompt}'")

        # åŠ è½½å¹¶å¤„ç†è¾“å…¥å›¾ç‰‡
        image = Image.open(BytesIO(image_bytes)).convert("RGB")
        print(f"åŸå§‹å›¾ç‰‡å°ºå¯¸: {image.size}")

        # è®¡ç®—è§†é¢‘å°ºå¯¸ï¼ˆä¿æŒåŸå›¾æ¯”ä¾‹ï¼‰
        width, height = self._calculate_video_size(image.width, image.height, max_area)
        print(f"è°ƒæ•´åå°ºå¯¸: {width}x{height}")
        
        # è°ƒæ•´å›¾ç‰‡å¤§å°
        image = image.resize((width, height))

        # è´Ÿå‘æç¤ºè¯ï¼ˆä¸­æ–‡ + è‹±æ–‡ï¼‰
        negative_prompt = (
            "è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œ"
            "æ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œ"
            "ç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œ"
            "æ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°"
        )

        # è®¾ç½®ç”Ÿæˆå™¨
        generator = torch.Generator(device=self.device).manual_seed(seed)

        # ç”Ÿæˆè§†é¢‘
        print(f"å¼€å§‹ç”Ÿæˆè§†é¢‘... åˆ†è¾¨ç‡: {width}x{height}, å¸§æ•°: {num_frames}")
        output = self.pipe(
            image=image,
            prompt=prompt,
            negative_prompt=negative_prompt,
            height=height,
            width=width,
            num_frames=num_frames,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
            generator=generator,
        ).frames[0]

        print(f"è§†é¢‘ç”Ÿæˆå®Œæˆï¼å¸§æ•°: {len(output)}")

        # å¯¼å‡ºä¸º MP4ï¼ˆI2V é»˜è®¤ä½¿ç”¨ 16fpsï¼‰
        print("æ­£åœ¨å¯¼å‡ºè§†é¢‘...")
        video_path = "/tmp/i2v_output.mp4"
        export_to_video(output, video_path, fps=16)

        # è¯»å–è§†é¢‘æ–‡ä»¶ä¸ºå­—èŠ‚æµ
        video_bytes = Path(video_path).read_bytes()
        print(f"è§†é¢‘æ–‡ä»¶å¤§å°: {len(video_bytes) / 1024 / 1024:.2f} MB")

        return video_bytes


@app.function(image=image, timeout=1800)
@modal.fastapi_endpoint(method="POST")
async def generate_video_api(
    image: UploadFile = File(...),
    prompt: str = Form(...),
    max_area: int = Form(480 * 832),  # 480P é»˜è®¤
    num_frames: int = Form(81),
    num_inference_steps: int = Form(40),
    guidance_scale: float = Form(3.5),
    seed: int = Form(0),
):
    """
    Web API ç«¯ç‚¹ï¼Œç”¨äºé€šè¿‡ HTTP POST è¯·æ±‚ç”Ÿæˆè§†é¢‘ã€‚
    
    ä½¿ç”¨ multipart/form-data æ ¼å¼ï¼š
    - image: è¾“å…¥å›¾ç‰‡æ–‡ä»¶ï¼ˆå¿…å¡«ï¼‰
    - prompt: æ–‡æœ¬æç¤ºè¯ï¼ˆå¿…å¡«ï¼‰
    - max_area: æœ€å¤§é¢ç§¯ï¼ˆé»˜è®¤ 480*832=480Pï¼Œå¯è®¾ç½® 720*1280=720Pï¼‰
    - num_frames: å¸§æ•°ï¼ˆé»˜è®¤ 81ï¼‰
    - num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆé»˜è®¤ 40ï¼‰
    - guidance_scale: å¼•å¯¼å¼ºåº¦ï¼ˆé»˜è®¤ 3.5ï¼‰
    - seed: éšæœºç§å­ï¼ˆé»˜è®¤ 0ï¼‰
    """
    print(f"æ”¶åˆ°æ¥è‡ª Web çš„è¯·æ±‚ï¼Œæç¤ºè¯: '{prompt}'")

    # è¯»å–ä¸Šä¼ çš„å›¾ç‰‡
    image_bytes = await image.read()
    print(f"æ”¶åˆ°è¾“å…¥å›¾ç‰‡ï¼Œå¤§å°: {len(image_bytes)} bytes")

    # è¿œç¨‹è°ƒç”¨æ ¸å¿ƒç”Ÿæˆå‡½æ•°
    video_bytes = Model().generate_video.remote(
        image_bytes=image_bytes,
        prompt=prompt,
        max_area=max_area,
        num_frames=num_frames,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        seed=seed,
    )

    # è¿”å›ç”Ÿæˆçš„è§†é¢‘
    return Response(content=video_bytes, media_type="video/mp4")


@app.local_entrypoint()
def main(
    image_path: str,
    prompt: str = "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard.",
    output_path: str = "/tmp/wan22-i2v/output.mp4",
    resolution: str = "480p",  # 480p æˆ– 720p
    num_frames: int = 81,
    num_inference_steps: int = 40,
    guidance_scale: float = 3.5,
    seed: int = 0,
):
    """
    æœ¬åœ°å…¥å£å‡½æ•°ï¼šè°ƒç”¨äº‘ç«¯æ¨¡å‹ç”Ÿæˆè§†é¢‘ï¼Œä¿å­˜ç»“æœã€‚
    
    ç”¨æ³•ç¤ºä¾‹:
    1. ç”Ÿæˆ 480P è§†é¢‘:
       modal run wan22_i2v_deploy.py --image-path ./cat.jpg \
           --prompt "ä¸€åªçŒ«åœ¨æµ·æ»©ä¸Šå†²æµª"
    
    2. ç”Ÿæˆ 720P è§†é¢‘:
       modal run wan22_i2v_deploy.py --image-path ./cat.jpg \
           --prompt "ä¸€åªçŒ«åœ¨æµ·æ»©ä¸Šå†²æµª" \
           --resolution 720p
    """
    input_image_path = Path(image_path)
    output_video_path = Path(output_path)

    if not input_image_path.exists():
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥å›¾ç‰‡ {input_image_path}")
        return

    # æ ¹æ®åˆ†è¾¨ç‡è®¾ç½®æœ€å¤§é¢ç§¯
    max_area = 720 * 1280 if resolution.lower() == "720p" else 480 * 832
    resolution_name = "720P" if resolution.lower() == "720p" else "480P"

    print(f"ğŸ¬ æ­£åœ¨è¯»å–è¾“å…¥å›¾ç‰‡: {input_image_path}")
    image_bytes = input_image_path.read_bytes()

    print(f"ğŸ¬ æ¨¡å¼: å›¾ç‰‡ç”Ÿæˆè§†é¢‘ (I2V)")
    print(f"ğŸ¬ åˆ†è¾¨ç‡: {resolution_name}")
    print(f"ğŸ¬ æç¤ºè¯: '{prompt}'")
    print(f"ğŸ¬ å¸§æ•°: {num_frames} ({num_frames/16:.1f}ç§’ @ 16fps)")
    print(f"ğŸ¬ æ­£åœ¨äº‘ç«¯ç”Ÿæˆè§†é¢‘ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")

    # è°ƒç”¨è¿œç¨‹ç”Ÿæˆå‡½æ•°
    video_bytes = Model().generate_video.remote(
        image_bytes=image_bytes,
        prompt=prompt,
        max_area=max_area,
        num_frames=num_frames,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        seed=seed,
    )

    # ä¿å­˜è§†é¢‘
    output_video_path.parent.mkdir(exist_ok=True, parents=True)
    print(f"ğŸ¬ æ­£åœ¨ä¿å­˜è§†é¢‘åˆ°: {output_video_path}")
    output_video_path.write_bytes(video_bytes)
    print(f"âœ… å®Œæˆï¼è§†é¢‘å·²ä¿å­˜åˆ°: {output_video_path}")