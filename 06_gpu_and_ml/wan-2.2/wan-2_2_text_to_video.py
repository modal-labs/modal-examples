# ---
# output-directory: "/tmp/wan22-video"
# ---

# # ä½¿ç”¨ Modal éƒ¨ç½² Wan2.2-TI2V-5B è§†é¢‘ç”Ÿæˆæ¨¡å‹

# åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨äº‘ç«¯GPUä¸Šè¿è¡Œé˜¿é‡Œçš„ Wan2.2-TI2V-5B æ¨¡å‹ã€‚
# è¿™æ˜¯ä¸€ä¸ªæ”¯æŒæ–‡æœ¬ç”Ÿæˆè§†é¢‘(T2V)å’Œå›¾ç‰‡ç”Ÿæˆè§†é¢‘(I2V)çš„æ··åˆæ¨¡å‹ï¼Œ
# å¯ä»¥ç”Ÿæˆ 720P@24fps çš„é«˜è´¨é‡è§†é¢‘ã€‚

# æ¨¡å‹ä¸»é¡µ: https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B
# GitHub: https://github.com/Wan-Video/Wan2.2

from io import BytesIO
from pathlib import Path
from typing import Optional

import modal
from fastapi import File, Form, UploadFile
from fastapi.responses import Response


# 1. å®šä¹‰å®¹å™¨é•œåƒï¼šå®‰è£…æ‰€æœ‰å¿…è¦çš„åº“
image = (
    modal.Image.from_registry(
        "nvidia/cuda:12.1.1-devel-ubuntu22.04",
        add_python="3.11",
    )
    .apt_install("git", "ffmpeg")  # ffmpeg ç”¨äºè§†é¢‘å¤„ç†
    .pip_install(
        "torch>=2.4.0",
        "torchvision",
        "transformers==4.44.2",  # å›ºå®šä¸€ä¸ªç¨³å®šç‰ˆæœ¬ï¼Œé¿å… offload_state_dict é—®é¢˜
        "diffusers>=0.34.0",  # ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ä»¥æ”¯æŒ Wan2.2-VAE
        "Pillow>=10.2.0",
        "huggingface-hub>=0.22.0",
        "accelerate>=0.29.0",
        "sentencepiece",  # T5 æ¨¡å‹éœ€è¦
        "protobuf",
        "ftfy",  # WanPipeline æ–‡æœ¬å¤„ç†éœ€è¦
        "fastapi",
        "python-multipart",
        "numpy",
        "imageio[ffmpeg]",  # è§†é¢‘å¯¼å‡º
        extra_index_url="https://download.pytorch.org/whl/cu121",
    )
)

# å®šä¹‰æ¨¡å‹åç§°å’Œç¼“å­˜è·¯å¾„
MODEL_NAME = "Wan-AI/Wan2.2-TI2V-5B-Diffusers"
CACHE_DIR = Path("/cache")

# åˆ›å»ºä¸€ä¸ªæŒä¹…åŒ–çš„å­˜å‚¨å·æ¥ç¼“å­˜æ¨¡å‹
cache_volume = modal.Volume.from_name("hf-hub-cache-wan22", create_if_missing=True)
volumes = {CACHE_DIR: cache_volume}

# ä»Modalå¹³å°å®‰å…¨åœ°è·å–HuggingFaceçš„APIå¯†é’¥
secrets = [modal.Secret.from_name("huggingface-secret")]

app = modal.App("example-wan22-ti2v-video-generation")

@app.cls(
    image=image,
    gpu="A100-80GB",  # Wan2.2-TI2V-5B åœ¨ 80GB GPU ä¸Šå¯ä»¥å¿«é€Ÿè¿è¡Œ
    volumes=volumes,
    secrets=secrets,
    timeout=1800,  # 30åˆ†é’Ÿè¶…æ—¶ï¼Œè§†é¢‘ç”Ÿæˆéœ€è¦è¾ƒé•¿æ—¶é—´
    scaledown_window=300,  # ä¿®å¤ï¼šä½¿ç”¨æ–°çš„å‚æ•°å
)
class Model:
    @modal.enter()
    def enter(self):
        """
        å®¹å™¨å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡ï¼šä¸‹è½½å¹¶åŠ è½½æ¨¡å‹åˆ°GPUã€‚
        """
        import torch
        from diffusers import WanPipeline

        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_NAME}")
        self.device = "cuda"
        self.dtype = torch.bfloat16

        # ç›´æ¥åŠ è½½å®Œæ•´çš„ Pipelineï¼Œè®©å®ƒè‡ªåŠ¨å¤„ç† VAE
        print("åŠ è½½ Wan Pipelineï¼ˆåŒ…å« VAEï¼‰...")
        try:
            # å°è¯•æ­£å¸¸åŠ è½½
            self.pipe = WanPipeline.from_pretrained(
                MODEL_NAME,
                torch_dtype=self.dtype,
                cache_dir=CACHE_DIR,
            )
        except ValueError as e:
            # å¦‚æœå‡ºç°å½¢çŠ¶ä¸åŒ¹é…é”™è¯¯ï¼Œä½¿ç”¨ä½å†…å­˜æ¨¡å¼å’Œå¿½ç•¥ä¸åŒ¹é…
            print(f"å¸¸è§„åŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å…¼å®¹æ¨¡å¼: {e}")
            self.pipe = WanPipeline.from_pretrained(
                MODEL_NAME,
                torch_dtype=self.dtype,
                cache_dir=CACHE_DIR,
                low_cpu_mem_usage=False,
                ignore_mismatched_sizes=True,
            )
        
        self.pipe.to(self.device)

        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")

    @modal.method()
    def generate_video(
        self,
        prompt: str,
        image_bytes: Optional[bytes] = None,
        height: int = 704,
        width: int = 1280,
        num_frames: int = 121,
        num_inference_steps: int = 50,
        guidance_scale: float = 5.0,
        seed: int = 42,
    ) -> bytes:
        """
        æ ¸å¿ƒçš„è§†é¢‘ç”Ÿæˆå‡½æ•°ã€‚
        
        å‚æ•°:
        - prompt: æ–‡æœ¬æç¤ºè¯
        - image_bytes: å¯é€‰çš„è¾“å…¥å›¾ç‰‡ï¼ˆå¦‚æœæä¾›åˆ™ä¸º I2Vï¼Œå¦åˆ™ä¸º T2Vï¼‰
        - height: è§†é¢‘é«˜åº¦ï¼ˆé»˜è®¤ 704ï¼‰
        - width: è§†é¢‘å®½åº¦ï¼ˆé»˜è®¤ 1280ï¼‰
        - num_frames: å¸§æ•°ï¼ˆé»˜è®¤ 121ï¼Œçº¦5ç§’@24fpsï¼‰
        - num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆé»˜è®¤ 50ï¼‰
        - guidance_scale: å¼•å¯¼å¼ºåº¦ï¼ˆé»˜è®¤ 5.0ï¼‰
        - seed: éšæœºç§å­
        
        è¿”å›:
        - ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶ï¼ˆMP4æ ¼å¼ï¼‰çš„å­—èŠ‚æµ
        """
        import torch
        from PIL import Image
        from diffusers.utils import export_to_video

        print(f"æ”¶åˆ°æ–°çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡")
        print(f"æç¤ºè¯: '{prompt}'")
        print(f"æ¨¡å¼: {'å›¾ç‰‡ç”Ÿæˆè§†é¢‘ (I2V)' if image_bytes else 'æ–‡æœ¬ç”Ÿæˆè§†é¢‘ (T2V)'}")
        print(f"åˆ†è¾¨ç‡: {width}x{height}, å¸§æ•°: {num_frames}")

        # è´Ÿå‘æç¤ºè¯ï¼ˆä¸­æ–‡ + è‹±æ–‡ï¼‰
        negative_prompt = (
            "è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œ"
            "æ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œ"
            "ç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œ"
            "æ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°"
        )

        # å¦‚æœæä¾›äº†å›¾ç‰‡ï¼Œåˆ™è¿›è¡Œ I2V ç”Ÿæˆ
        image = None
        if image_bytes:
            image = Image.open(BytesIO(image_bytes)).convert("RGB")
            print(f"è¾“å…¥å›¾ç‰‡å°ºå¯¸: {image.size}")

        # è®¾ç½®ç”Ÿæˆå™¨
        generator = torch.Generator(device=self.device).manual_seed(seed)

        # ç”Ÿæˆè§†é¢‘
        print("å¼€å§‹ç”Ÿæˆè§†é¢‘...")
# æ›¿æ¢ä½ åŸæ¥çš„ output = self.pipe(...) éƒ¨åˆ†

        if image_bytes:
            # å¦‚æœæä¾›å›¾ç‰‡ï¼Œåˆ™åŠ è½½ I2V æ¨¡å‹
            from diffusers import WanI2VPipeline
            i2v_pipe = WanI2VPipeline.from_pretrained(
                MODEL_NAME.replace("TI2V", "I2V"),  # é€šå¸¸ I2V æ¨¡å‹æœ‰ç‹¬ç«‹æƒé‡
                torch_dtype=self.dtype,
                cache_dir=CACHE_DIR,
            ).to(self.device)

            output = i2v_pipe(
                image=image,
                prompt=prompt,
                negative_prompt=negative_prompt,
                height=height,
                width=width,
                num_frames=num_frames,
                guidance_scale=guidance_scale,
                num_inference_steps=num_inference_steps,
                generator=generator,
            ).frames[0]
        else:
            # å¦åˆ™ä½¿ç”¨ T2V pipeline
            output = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                height=height,
                width=width,
                num_frames=num_frames,
                guidance_scale=guidance_scale,
                num_inference_steps=num_inference_steps,
                generator=generator,
            ).frames[0]

        print(f"è§†é¢‘ç”Ÿæˆå®Œæˆï¼å¸§æ•°: {len(output)}")

        # å¯¼å‡ºä¸º MP4
        print("æ­£åœ¨å¯¼å‡ºè§†é¢‘...")
        video_path = "/tmp/output_video.mp4"
        export_to_video(output, video_path, fps=24)

        # è¯»å–è§†é¢‘æ–‡ä»¶ä¸ºå­—èŠ‚æµ
        video_bytes = Path(video_path).read_bytes()
        print(f"è§†é¢‘æ–‡ä»¶å¤§å°: {len(video_bytes) / 1024 / 1024:.2f} MB")

        return video_bytes


@app.function(image=image, timeout=1800)
@modal.fastapi_endpoint(method="POST")  # ä¿®å¤ï¼šä½¿ç”¨æ–°çš„è£…é¥°å™¨åç§°
async def generate_video_api(
    prompt: str = Form(...),
    image: Optional[UploadFile] = File(None),
    height: int = Form(704),
    width: int = Form(1280),
    num_frames: int = Form(121),
    num_inference_steps: int = Form(50),
    guidance_scale: float = Form(5.0),
    seed: int = Form(42),
):
    """
    Web API ç«¯ç‚¹ï¼Œç”¨äºé€šè¿‡ HTTP POST è¯·æ±‚ç”Ÿæˆè§†é¢‘ã€‚
    
    ä½¿ç”¨ multipart/form-data æ ¼å¼ï¼š
    - prompt: æ–‡æœ¬æç¤ºè¯ï¼ˆå¿…å¡«ï¼‰
    - image: å¯é€‰çš„è¾“å…¥å›¾ç‰‡æ–‡ä»¶
    - height: è§†é¢‘é«˜åº¦ï¼ˆé»˜è®¤ 704ï¼‰
    - width: è§†é¢‘å®½åº¦ï¼ˆé»˜è®¤ 1280ï¼‰
    - num_frames: å¸§æ•°ï¼ˆé»˜è®¤ 121ï¼‰
    - num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆé»˜è®¤ 50ï¼‰
    - guidance_scale: å¼•å¯¼å¼ºåº¦ï¼ˆé»˜è®¤ 5.0ï¼‰
    - seed: éšæœºç§å­ï¼ˆé»˜è®¤ 42ï¼‰
    """
    print(f"æ”¶åˆ°æ¥è‡ª Web çš„è¯·æ±‚ï¼Œæç¤ºè¯: '{prompt}'")

    # è¯»å–ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
    image_bytes = None
    if image:
        image_bytes = await image.read()
        print(f"æ”¶åˆ°è¾“å…¥å›¾ç‰‡ï¼Œå¤§å°: {len(image_bytes)} bytes")

    # è¿œç¨‹è°ƒç”¨æ ¸å¿ƒç”Ÿæˆå‡½æ•°
    video_bytes = Model().generate_video.remote(
        prompt=prompt,
        image_bytes=image_bytes,
        height=height,
        width=width,
        num_frames=num_frames,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        seed=seed,
    )

    # è¿”å›ç”Ÿæˆçš„è§†é¢‘
    return Response(content=video_bytes, media_type="video/mp4")


@app.local_entrypoint()
def main(
    prompt: str = "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage",
    image_path: Optional[str] = None,
    output_path: str = "/tmp/wan22-video/output.mp4",
    height: int = 704,
    width: int = 1280,
    num_frames: int = 121,
    num_inference_steps: int = 50,
    guidance_scale: float = 5.0,
    seed: int = 42,
):
    """
    æœ¬åœ°å…¥å£å‡½æ•°ï¼šè°ƒç”¨äº‘ç«¯æ¨¡å‹ç”Ÿæˆè§†é¢‘ï¼Œä¿å­˜ç»“æœã€‚
    
    ç”¨æ³•ç¤ºä¾‹:
    1. æ–‡æœ¬ç”Ÿæˆè§†é¢‘ (T2V):
       modal run wan22_deploy.py --prompt "ä¸€åªå¯çˆ±çš„ç†ŠçŒ«åœ¨ç«¹æ—é‡Œç©è€"
    
    2. å›¾ç‰‡ç”Ÿæˆè§†é¢‘ (I2V):
       modal run wan22_deploy.py --prompt "è¿™åªçŒ«åœ¨æµ·æ»©ä¸Šå†²æµª" --image-path ./cat.jpg
    """
    output_video_path = Path(output_path)

    # è¯»å–è¾“å…¥å›¾ç‰‡ï¼ˆå¦‚æœæä¾›ï¼‰
    image_bytes = None
    if image_path:
        input_image_path = Path(image_path)
        if not input_image_path.exists():
            print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥å›¾ç‰‡ {input_image_path}")
            return
        print(f"ğŸ¬ æ­£åœ¨è¯»å–è¾“å…¥å›¾ç‰‡: {input_image_path}")
        image_bytes = input_image_path.read_bytes()

    mode = "å›¾ç‰‡ç”Ÿæˆè§†é¢‘ (I2V)" if image_bytes else "æ–‡æœ¬ç”Ÿæˆè§†é¢‘ (T2V)"
    print(f"ğŸ¬ æ¨¡å¼: {mode}")
    print(f"ğŸ¬ æç¤ºè¯: '{prompt}'")
    print(f"ğŸ¬ åˆ†è¾¨ç‡: {width}x{height}")
    print(f"ğŸ¬ å¸§æ•°: {num_frames} ({num_frames/24:.1f}ç§’ @ 24fps)")
    print(f"ğŸ¬ æ­£åœ¨äº‘ç«¯ç”Ÿæˆè§†é¢‘ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")

    # è°ƒç”¨è¿œç¨‹ç”Ÿæˆå‡½æ•°
    video_bytes = Model().generate_video.remote(
        prompt=prompt,
        image_bytes=image_bytes,
        height=height,
        width=width,
        num_frames=num_frames,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        seed=seed,
    )

    # ä¿å­˜è§†é¢‘
    output_video_path.parent.mkdir(exist_ok=True, parents=True)
    print(f"ğŸ¬ æ­£åœ¨ä¿å­˜è§†é¢‘åˆ°: {output_video_path}")
    output_video_path.write_bytes(video_bytes)
    print(f"âœ… å®Œæˆï¼è§†é¢‘å·²ä¿å­˜åˆ°: {output_video_path}")