# ---
# output-directory: "/tmp/wan22-t2v-a14b"
# ---

# # ä½¿ç”¨ Modal éƒ¨ç½² Wan2.2-T2V-A14B æ–‡å­—ç”Ÿæˆè§†é¢‘æ¨¡å‹

# Wan2.2-T2V-A14B æ˜¯ä¸“ä¸šçš„æ–‡æœ¬ç”Ÿæˆè§†é¢‘ MoE æ¨¡å‹ã€‚
# é‡‡ç”¨åŒä¸“å®¶ MoE æ¶æ„ï¼šæ€»å‚æ•° 27Bï¼Œæ¯æ­¥æ¿€æ´» 14Bã€‚
# æ”¯æŒ 720P é«˜æ¸…è§†é¢‘ç”Ÿæˆï¼Œç”Ÿæˆè´¨é‡è¾¾åˆ°å•†ä¸šçº§æ°´å¹³ã€‚

# æ¨¡å‹ä¸»é¡µ: https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B-Diffusers
# GitHub: https://github.com/Wan-Video/Wan2.2

from io import BytesIO
from pathlib import Path

import modal
from fastapi import Form
from fastapi.responses import Response


# 1. å®šä¹‰å®¹å™¨é•œåƒï¼šå®‰è£…æ‰€æœ‰å¿…è¦çš„åº“
image = (
    modal.Image.from_registry(
        "nvidia/cuda:12.1.1-devel-ubuntu22.04",
        add_python="3.11",
    )
    .apt_install("git", "ffmpeg")
    .pip_install(
        "torch>=2.4.0",
        "torchvision",
        "transformers==4.44.2",  # ç¨³å®šç‰ˆæœ¬
        "diffusers>=0.34.0",  # æ”¯æŒ Wan2.2
        "Pillow>=10.2.0",
        "huggingface-hub>=0.22.0",
        "accelerate>=0.29.0",
        "sentencepiece",
        "protobuf",
        "ftfy",
        "fastapi",
        "python-multipart",
        "numpy",
        "imageio[ffmpeg]",
        extra_index_url="https://download.pytorch.org/whl/cu121",
    )
)

# å®šä¹‰æ¨¡å‹åç§°å’Œç¼“å­˜è·¯å¾„
MODEL_NAME = "Wan-AI/Wan2.2-Animate-14B"
CACHE_DIR = Path("/cache")

# åˆ›å»ºæŒä¹…åŒ–å­˜å‚¨å·
cache_volume = modal.Volume.from_name("hf-hub-cache-wan22-t2v-a14b", create_if_missing=True)
volumes = {CACHE_DIR: cache_volume}

# HuggingFace API å¯†é’¥
secrets = [modal.Secret.from_name("huggingface-secret")]

app = modal.App("example-wan22-t2v-a14b-video-generation")

@app.cls(
    image=image,
    gpu="H100",  # H100 æ˜¯æœ€ä½³é€‰æ‹©ï¼šæ›´å¿«çš„é€Ÿåº¦ + 80GB æ˜¾å­˜
    volumes=volumes,
    secrets=secrets,
    timeout=2400,
    scaledown_window=300,
)
class Model:
    @modal.enter()
    def enter(self):
        """
        å®¹å™¨å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡ï¼šä¸‹è½½å¹¶åŠ è½½æ¨¡å‹åˆ°GPUã€‚
        """
        import torch
        from diffusers import WanPipeline

        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_NAME}")
        self.device = "cuda"
        self.dtype = torch.bfloat16

        # åŠ è½½ Text-to-Video Pipeline
        print("åŠ è½½ Wan T2V Pipeline...")
        try:
            self.pipe = WanPipeline.from_pretrained(
                MODEL_NAME,
                torch_dtype=self.dtype,
                cache_dir=CACHE_DIR,
            )
        except ValueError as e:
            print(f"å¸¸è§„åŠ è½½å¤±è´¥ï¼Œå°è¯•å…¼å®¹æ¨¡å¼: {e}")
            self.pipe = WanPipeline.from_pretrained(
                MODEL_NAME,
                torch_dtype=self.dtype,
                cache_dir=CACHE_DIR,
                low_cpu_mem_usage=False,
                ignore_mismatched_sizes=True,
            )
        
        self.pipe.to(self.device)

        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print(f"æ¨¡å‹: Wan2.2-T2V-A14B (MoE 27B/14Bæ¿€æ´»)")

    @modal.method()
    def generate_video(
        self,
        prompt: str,
        height: int = 720,
        width: int = 1280,
        num_frames: int = 81,  # é»˜è®¤ 81 å¸§ï¼ˆçº¦ 5 ç§’ @ 16fpsï¼‰
        num_inference_steps: int = 50,
        guidance_scale: float = 5.0,
        seed: int = 42,
    ) -> bytes:
        """
        æ–‡æœ¬ç”Ÿæˆè§†é¢‘çš„æ ¸å¿ƒå‡½æ•°ã€‚
        
        å‚æ•°:
        - prompt: æ–‡æœ¬æç¤ºè¯ï¼ˆå¿…å¡«ï¼‰
        - height: è§†é¢‘é«˜åº¦ï¼ˆé»˜è®¤ 720ï¼‰
        - width: è§†é¢‘å®½åº¦ï¼ˆé»˜è®¤ 1280ï¼Œ720Pï¼‰
        - num_frames: å¸§æ•°ï¼ˆé»˜è®¤ 81ï¼Œçº¦ 5 ç§’ @ 16fpsï¼‰
        - num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆé»˜è®¤ 50ï¼‰
        - guidance_scale: å¼•å¯¼å¼ºåº¦ï¼ˆé»˜è®¤ 5.0ï¼‰
        - seed: éšæœºç§å­
        
        è¿”å›:
        - ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶ï¼ˆMP4æ ¼å¼ï¼‰çš„å­—èŠ‚æµ
        """
        import torch
        from diffusers.utils import export_to_video

        print(f"æ”¶åˆ°æ–‡æœ¬ç”Ÿæˆè§†é¢‘ä»»åŠ¡")
        print(f"æç¤ºè¯: '{prompt}'")
        print(f"åˆ†è¾¨ç‡: {width}x{height}")
        print(f"å¸§æ•°: {num_frames}")

        # è´Ÿå‘æç¤ºè¯ï¼ˆä¸­æ–‡ + è‹±æ–‡ï¼‰
        negative_prompt = (
            "è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œ"
            "æ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œ"
            "ç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œ"
            "æ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°"
        )

        # è®¾ç½®ç”Ÿæˆå™¨
        generator = torch.Generator(device=self.device).manual_seed(seed)

        # ç”Ÿæˆè§†é¢‘
        print("å¼€å§‹ç”Ÿæˆè§†é¢‘...")
        output = self.pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            height=height,
            width=width,
            num_frames=num_frames,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
            generator=generator,
        ).frames[0]

        print(f"è§†é¢‘ç”Ÿæˆå®Œæˆï¼å¸§æ•°: {len(output)}")

        # å¯¼å‡ºä¸º MP4ï¼ˆT2V ä½¿ç”¨ 16fpsï¼‰
        print("æ­£åœ¨å¯¼å‡ºè§†é¢‘...")
        video_path = "/tmp/t2v_output.mp4"
        export_to_video(output, video_path, fps=16)

        # è¯»å–è§†é¢‘æ–‡ä»¶ä¸ºå­—èŠ‚æµ
        video_bytes = Path(video_path).read_bytes()
        print(f"è§†é¢‘æ–‡ä»¶å¤§å°: {len(video_bytes) / 1024 / 1024:.2f} MB")

        return video_bytes


@app.function(image=image, timeout=1800)
@modal.fastapi_endpoint(method="POST")
async def generate_video_api(
    prompt: str = Form(...),
    height: int = Form(720),
    width: int = Form(1280),
    num_frames: int = Form(81),
    num_inference_steps: int = Form(50),
    guidance_scale: float = Form(5.0),
    seed: int = Form(42),
):
    """
    Web API ç«¯ç‚¹ï¼Œç”¨äºé€šè¿‡ HTTP POST è¯·æ±‚ç”Ÿæˆè§†é¢‘ã€‚
    
    ä½¿ç”¨ application/x-www-form-urlencoded æˆ– multipart/form-dataï¼š
    - prompt: æ–‡æœ¬æç¤ºè¯ï¼ˆå¿…å¡«ï¼‰
    - height: è§†é¢‘é«˜åº¦ï¼ˆé»˜è®¤ 720ï¼‰
    - width: è§†é¢‘å®½åº¦ï¼ˆé»˜è®¤ 1280ï¼‰
    - num_frames: å¸§æ•°ï¼ˆé»˜è®¤ 81ï¼‰
    - num_inference_steps: æ¨ç†æ­¥æ•°ï¼ˆé»˜è®¤ 50ï¼‰
    - guidance_scale: å¼•å¯¼å¼ºåº¦ï¼ˆé»˜è®¤ 5.0ï¼‰
    - seed: éšæœºç§å­ï¼ˆé»˜è®¤ 42ï¼‰
    """
    print(f"æ”¶åˆ°æ¥è‡ª Web çš„è¯·æ±‚ï¼Œæç¤ºè¯: '{prompt}'")

    # è¿œç¨‹è°ƒç”¨æ ¸å¿ƒç”Ÿæˆå‡½æ•°
    video_bytes = Model().generate_video.remote(
        prompt=prompt,
        height=height,
        width=width,
        num_frames=num_frames,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        seed=seed,
    )

    # è¿”å›ç”Ÿæˆçš„è§†é¢‘
    return Response(content=video_bytes, media_type="video/mp4")


@app.local_entrypoint()
def main(
    prompt: str = "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage",
    output_path: str = "/tmp/wan22-t2v-a14b/output.mp4",
    height: int = 720,
    width: int = 1280,
    num_frames: int = 81,
    num_inference_steps: int = 50,
    guidance_scale: float = 5.0,
    seed: int = 42,
):
    """
    æœ¬åœ°å…¥å£å‡½æ•°ï¼šè°ƒç”¨äº‘ç«¯æ¨¡å‹ç”Ÿæˆè§†é¢‘ï¼Œä¿å­˜ç»“æœã€‚
    
    ç”¨æ³•ç¤ºä¾‹:
    1. åŸºç¡€ä½¿ç”¨:
       modal run wan22_t2v_a14b_deploy.py \
           --prompt "ä¸€åªå¯çˆ±çš„ç†ŠçŒ«åœ¨ç«¹æ—é‡Œç©è€"
    
    2. è‡ªå®šä¹‰å‚æ•°:
       modal run wan22_t2v_a14b_deploy.py \
           --prompt "æ—¥è½æ—¶åˆ†ï¼ŒåŸå¸‚è¡—é“ä¸Šè½¦æ°´é©¬é¾™" \
           --num-frames 121 \
           --num-inference-steps 60 \
           --guidance-scale 6.0
    
    3. é«˜è´¨é‡é•¿è§†é¢‘:
       modal run wan22_t2v_a14b_deploy.py \
           --prompt "å£®ä¸½çš„å±±å·é£æ™¯ï¼Œäº‘é›¾ç¼­ç»•" \
           --num-frames 161 \
           --num-inference-steps 80
    """
    output_video_path = Path(output_path)

    print(f"ğŸ¬ æ¨¡å¼: æ–‡æœ¬ç”Ÿæˆè§†é¢‘ (T2V)")
    print(f"ğŸ¬ æ¨¡å‹: Wan2.2-T2V-A14B (MoE 27B/14B)")
    print(f"ğŸ¬ æç¤ºè¯: '{prompt}'")
    print(f"ğŸ¬ åˆ†è¾¨ç‡: {width}x{height} (720P)")
    print(f"ğŸ¬ å¸§æ•°: {num_frames} ({num_frames/16:.1f}ç§’ @ 16fps)")
    print(f"ğŸ¬ æ¨ç†æ­¥æ•°: {num_inference_steps}")
    print(f"ğŸ¬ å¼•å¯¼å¼ºåº¦: {guidance_scale}")
    print(f"ğŸ¬ æ­£åœ¨äº‘ç«¯ç”Ÿæˆè§†é¢‘ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")

    # è°ƒç”¨è¿œç¨‹ç”Ÿæˆå‡½æ•°
    video_bytes = Model().generate_video.remote(
        prompt=prompt,
        height=height,
        width=width,
        num_frames=num_frames,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        seed=seed,
    )

    # ä¿å­˜è§†é¢‘
    output_video_path.parent.mkdir(exist_ok=True, parents=True)
    print(f"ğŸ¬ æ­£åœ¨ä¿å­˜è§†é¢‘åˆ°: {output_video_path}")
    output_video_path.write_bytes(video_bytes)
    print(f"âœ… å®Œæˆï¼è§†é¢‘å·²ä¿å­˜åˆ°: {output_video_path}")